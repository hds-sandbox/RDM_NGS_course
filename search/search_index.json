{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"RDM for NGS data workshop","text":"<p>Updated: November 30, 2023</p> <p>Research Data Management (RDM) for Next Generation Sequencing (NGS) data is a foundational course aimed at providing participants with fundamental knowledge and practical skills in handling the extensive data generated through modern NGS studies in the context of Open Science and FAIR principles. This course covers essential principles of RDM practices, such as data organization, metadata annotation, version control and archiving, enabling researchers to manage NGS data with confidence. Participants will also gain insights into FAIR principles and Open Science, fostering collaboration and reproducibility in NGS research. By the end of the course, attendees will be equipped with essential tools and techniques to navigate the data challenges prevalent in today's NGS research landscape.</p> <p></p> Authors <p>J.A. Romero Herrera</p> <p>  Data Scientist </p> <p>Overview</p> <p> Syllabus:</p> <ol> <li>What is Research Data Management and why it is important</li> <li>What is NGS data</li> <li>Data Life Cycle</li> <li>Open Science and FAIR principles</li> <li>Data Management plans</li> <li>Folder and file structures applied to NGS data</li> <li>Metadata applied to NGS data</li> <li>Create a database of your data and projects</li> <li>Version control of your data analysis</li> <li>Archiving and repositories</li> </ol> <p> Total Time Estimation: X hours  </p> <p> Supporting Materials: </p> <p> Target Audience: PhD, MsC, anyone interested in RDM for NGS data.</p> <p> Level: Beginner.</p> <p> License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.</p> <p> Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).</p> <p>Course Requirements</p> <ul> <li>Basic understanding Next Generation Sequencing data and formats.</li> <li>Command Line experience</li> <li>Basic programming experience</li> <li>Mkdocs and mkdocs material</li> </ul> <p>This course provides participants with an overall introduction to effectively manage the vast amounts of data generated in modern NGS studies. Participants will gain a practical understanding of RDM principles and the significance of handling NGS data efficiently. The course covers the unique characteristics of NGS data, its life cycle, and the importance of adopting Open Science and FAIR principles for data accessibility and reusability.</p> <p>Throughout the course, participants will learn useful skills for organizing NGS data, including creating folder and file structures and implementing metadata to enhance data discoverability and interpretation. Data management plans (DMPs) tailored to NGS data will be explored, ensuring data integrity and compliance with institutional and funding agency requirements. Attendees will also gain insights into setting up simple databases and using version control systems to track changes in data analysis, promoting collaboration and reproducibility.</p> <p>The course concludes with a focus on archiving and data repositories, enabling participants to preserve and share NGS data for long-term scientific usage. By the end of the course, attendees will be equipped with the necessary tools and techniques to navigate the challenges prevalent in today's NGS research landscape, fostering successful data management practices and enhancing collaboration in the scientific community.</p> <p>Goals</p> <p>By the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:</p> <ul> <li>Understand what is RDM and why it is important</li> <li>Understand FAIR and Open Science Principles</li> <li>Write a Data Management Plan for your NGS data</li> <li>Structure and establish naming conventions for your files and folders</li> <li>Add relevant metadata to your data</li> <li>Version control your data analysis</li> <li>Select a repository to archive your data</li> <li>Make your data analysis and workflows reproducible</li> </ul>"},{"location":"index.html#research-data-management-for-ngs-data","title":"Research Data Management for NGS data","text":""},{"location":"index.html#acknowledgements","title":"Acknowledgements","text":"<ul> <li>University of Copenhagen Research Data Management Team.</li> <li>Martin Proks and Sarah Lundregan, Brickman Lab, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.</li> <li>Richard Dennis, Data Steward, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.</li> <li>NBISweden.</li> <li>RDMkit, Elixir Research Data Management Platform.</li> </ul>"},{"location":"index.html#feedback-form","title":"Feedback form","text":"<p>We would greatly appreciate to know your thoughts about this workshop. Please, follow this link to answer 9 fast questions!</p>"},{"location":"01_RDM_intro.html","title":"RDM intro","text":"","tags":["RDM","Bad RDM","Good RDM"]},{"location":"01_RDM_intro.html#introduction-to-research-data-management","title":"Introduction to Research Data Management","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Learn the basics about what is Research Data Management</li> <li>Learn why good RDM are important</li> </ol> <p>The University of Copenhagen (UCPH) defines Research Data Management (RDM) as a \"collective term for the planning, collection, storage, sharing and preservation of research data\"<sup>1</sup>.</p> <p>Proper RDM practices have great importance in modern scientific research due to the exponential growth of data in various disciplines, particularly in fields like genomics, climate research, and social sciences. Effectively managing research data offers many benefits! For example:</p> <ol> <li>It enhances the reliability and credibility of research findings by providing a transparent and structured approach to data collection, organization, and analysis.</li> <li>RDM promotes data discoverability, allowing researchers to find and use existing data for new investigations, thus maximizing the value of the research.</li> <li>Proper data management facilitates collaboration and knowledge sharing within the scientific community, accelerating the pace of discoveries and advancements.</li> <li>RDM ensures compliance with ethical and legal requirements, safeguarding sensitive data and promoting responsible data use.</li> </ol> <p>By adopting robust RDM practices, researchers can optimize their workflow, increase research impact, and contribute to the overall progress of science. In the next sections, we will dive deeper into the meaning of RDM and its benefits.</p>","tags":["RDM","Bad RDM","Good RDM"]},{"location":"01_RDM_intro.html#what-is-research-data-management","title":"What is Research Data Management","text":"<p>While the meaning of Research Data Management might be obvious, it is a good idea to break down its components to make a good sense of what it implies. Let's start with Management, which is rather simple!</p> <p>Meaning of Management</p> <p>The literal meaning of Management is \"the practice of managing; handling, supervision, or control.\"</p> <p>On the other hand, we have Research Data:</p> <p>Meaning of Research Data</p> <p>In accordance with the UCPH Policy for Research Data Management<sup>1</sup>, research data encompasses both physical material and digital information gathered, observed, produced, or formulated during research activities carried out at UCPH. This broad definition includes various types of data serving as the foundation for the research, such as specimens, notebooks, interviews, texts, literature, digital raw data, recordings, computer code, and meticulous documentation of these materials and data, forming the core of the analysis that underlies the research outcomes.</p> <p>So, our goal is to handle and control the data that we generate during our research, including both physical and digital data! This must be done throughout the whole life cycle of the data.</p> <p>Nonetheless, since this is a workshop on bioinformatics data, NGS to be specific, we will focus on the digital part of our data. We will not talk much about wet lab RDM, such as protocols, instruments, reagents, ELM or LIMS systems, although this would ideally be integrated as part of the whole RDM process.</p> <p>Warning</p> <p>This workshop focuses on RDM of digital data and how it was generated.</p>","tags":["RDM","Bad RDM","Good RDM"]},{"location":"01_RDM_intro.html#research-data-cycle","title":"Research Data Cycle","text":"<p>The Research Data Life Cycle is a conceptual framework that illustrates the various stages that research data goes through during its lifetime, from its initial creation or collection to its eventual archiving or disposal. It provides a structured approach to managing research data effectively, ensuring data integrity, accessibility, and reusability. We will talk about the Research Data Life Cycle in the third lesson.</p> <p> Research Data Life Cycle, University of Copenhagen RDM guidelines. </p>","tags":["RDM","Bad RDM","Good RDM"]},{"location":"01_RDM_intro.html#why-is-research-data-management-important","title":"Why is Research Data Management important","text":"<p>Effective data management can significantly benefit research, providing advantages for individual researchers:</p> <ul> <li>Careful planning helps in the early identification and resolution of potential issues, aligns expectations among collaborators, and clarifies data rights and ownership.</li> <li>Clear data documentation streamlines the process of locating and comprehending previous research, promoting efficiency and building upon existing knowledge.</li> <li>Conducting risk assessments and devising robust data storage and security strategies mitigate the risk of data loss, breaches, or unauthorized use, safeguarding valuable research.</li> <li>Sharing data with others beyond the project's conclusion enhances research visibility and fosters increased citations, expanding the impact of your findings.</li> <li>Developing a data preservation plan ensures the long-term availability of research data well after the project's completion, contributing to data accessibility and continued research relevance.</li> </ul>","tags":["RDM","Bad RDM","Good RDM"]},{"location":"01_RDM_intro.html#the-cost-of-bad-rdm-practices","title":"The cost of bad RDM practices","text":"<p>Several surveys have shown that data scientists spend between 40-60% of their time loading and cleaning data, becoming the most consuming, and what many would call tedious, tasks of their jobs<sup>2</sup><sup>3</sup>. Below we show the number figures from the Anaconda \"State of data science 2020\" report<sup>2</sup>.</p> <p> Time spent in different tasks by data scientists, Anaconda State of data science report 2020. </p> <p>Could you think why we spend so much time? Maybe this pictures look familiar to you...</p> <p> Photo by Wonderlane on Unsplash. </p> <p> Messy folder structure, by my old-self. </p> <p>Or ever felt like this?</p> <p> From Stanford Center for Reproducible Neuroscience. </p> <p>What about these situations? Have you ever encounter any?</p> <ol> <li>Imagine a researcher working on a project without a clear folder structure or meaningful file names for their data. As the project progresses and data accumulates, it becomes challenging for the researcher to locate specific data files quickly. This leads to wasted time searching for relevant information and delays in data analysis.</li> <li>Suppose a researcher does not document their data collection methods adequately. When another researcher attempts to analyze the data later, they struggle to understand the context in which the data was gathered, leading to misinterpretations and errors in the analysis.</li> <li>A researcher publishes a groundbreaking study, but their data and methods lack proper documentation and are not made available to others. As a result, other researchers find it challenging or impossible to reproduce the study's results, leading to doubts about the validity of the findings.</li> <li>Suppose a research team spends weeks trying to clean and validate poorly organized data before they can start their analysis. This wasted time and effort could have been better spent on more productive research tasks.</li> </ol> <p>Bad data management practices can have significant consequences that affect both your future self and colleagues who may have to deal with your data, as well as those handling other people's data. The implications of poor data management include:</p> <ul> <li>Difficulty in Data Retrieval: Without proper organization and documentation, finding specific data files or understanding their content becomes challenging and time-consuming. This leads to inefficiency and frustration when attempting to retrieve relevant information.</li> <li>Loss of Data: Inadequate data backup and storage strategies increase the risk of data loss due to hardware failures, accidental deletions, or other unforeseen events. Losing valuable research data can be devastating and may result in the loss of months or even years of work.</li> <li>Data Incompleteness and Errors: Insufficient data documentation can lead to ambiguity and errors in data interpretation and analysis. This can undermine the credibility and reliability of research outcomes.</li> <li>Difficulty in Reproducibility: Inability to reproduce research results due to poor data management hinders scientific progress and challenges the validity of research findings.</li> <li>Delayed or Compromised Collaboration: In collaborative research projects, disorganized or poorly managed data can slow down progress and hinder effective communication among team members.</li> <li>Data Security and Privacy Risks: Inadequate data security measures can result in data breaches, compromising the confidentiality of sensitive information and exposing researchers and subjects to potential risks.</li> <li>Wasted Time and Resources: Poor data management practices necessitate additional time and effort to clean, validate, and organize data, diverting valuable resources from actual research tasks.</li> <li>Financial Implications: Time-consuming data management tasks translate to increased labor costs and potential project delays. Additionally, data loss may require costly data recovery attempts or, in extreme cases, data reconstruction.</li> <li>Reputational Damage: Inaccurate or irreproducible research outcomes can damage a researcher's reputation and credibility within the scientific community.</li> </ul> <p>By not practicing proper data management, researchers risk impeding their own progress and the progress of others who rely on their data. Furthermore, dealing with poorly managed data from others can be equally time-consuming and resource-intensive. To address these challenges, researchers could prioritize effective data management practices, including proper data organization, documentation, backup strategies, and adherence to data security and preservation protocols. Investing time and effort into good data management can prevent unnecessary setbacks, ensure data integrity, and ultimately contribute to more robust and reliable scientific research.</p>","tags":["RDM","Bad RDM","Good RDM"]},{"location":"01_RDM_intro.html#benefits-of-good-rdm-practices","title":"Benefits of good RDM practices","text":"<p>Let's see what could we have done to avoid or fix the issues we have previously mentioned:</p> <ol> <li>To avoid this, the researcher could have implemented a clear and consistent folder structure with descriptive file names. Additionally, using version control systems, such as Git, for code and analysis files can help track changes and facilitate easy retrieval of previous versions of analyses and results.</li> <li>Proper data documentation, including detailed metadata, could have been maintained throughout the data collection process, providing necessary context and reducing the risk of incomplete or ambiguous data.</li> <li>The researcher could have followed the FAIR principles (Findable, Accessible, Interoperable, Reusable) by making their data, along with detailed methods and documentation, openly accessible in a reputable data repository.</li> <li>By implementing data management strategies from the outset of the research project, researchers can save time and resources later on, ensuring that data is well-organized and properly documented.</li> </ol> <p>All in all, good data management absolutely have a positive impact on research. For individual researchers:</p> <ul> <li>Good planning can help identify issues before they become a problem, and help align expectations between collaborators, for examples concerning rights to data.</li> <li>Good data documentation can facilitate finding and understanding past research.</li> <li>Conducting risk assessments and making a strategy for data storage and security can prevent data loss, data breaches or data misuse.</li> <li>Sharing data with others after project end can enhance the visibility of research and lead to an increase in citations.</li> <li>Drafting a data preservation plan can help ensure the availability of research data for years after the end of the project.\u200b</li> </ul>","tags":["RDM","Bad RDM","Good RDM"]},{"location":"01_RDM_intro.html#wrap-up","title":"Wrap up","text":"<p>In this lesson we have learned about what is RDM, the consequences of poor RDM practices and how following good RDM practices will help you to to avoid them! In the next lessons we will explore different resources, tools and guidelines that can be applied to all kinds of data and how to apply it specifically for NGS data.</p> <ol> <li> <p>Research Data Management Team. Policy for research data management. 2022.\u00a0\u21a9\u21a9</p> </li> <li> <p>The state of data science 2020 moving from hype toward maturity. 2020. URL: https://www.anaconda.com/resources/whitepapers/state-of-data-science-2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>Cleaning big data: most time-consuming, least enjoyable data science task, survey says. 2016. URL: https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/.\u00a0\u21a9</p> </li> </ol>","tags":["RDM","Bad RDM","Good RDM"]},{"location":"02_NGS_data.html","title":"Next Generation Sequencing Data","text":"","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#next-generation-sequencing-data","title":"Next Generation Sequencing data","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Learn about what is Next Generation Sequencing  </li> <li>Learn about different types of NGS data</li> <li>Learn about the specific challenges regarding RDM for NGS data</li> </ol> <p>In this session, we'll explore the essentials of Next Generation Sequencing technology, which generates vast data volumes in a single experiment. We'll examine the types of data produced during an NGS experiment and the results created from the downstream analysis. Moreover, we'll address specific challenges NGS encounters in RDM, such as, managing genomic resources, pipelines/workflows, data analysis, data preservation and documentation. We will also mention briefly concerns regarding sensitive data and GDPR protection of NGS data.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#what-is-next-generation-sequencing","title":"What is Next Generation Sequencing","text":"<p>Next Generation Sequencing (NGS), also known as high-throughput sequencing, is a revolutionary technology that has transformed genomics research in recent years. NGS is a suite of advanced DNA sequencing techniques that enable rapid and cost-effective analysis of DNA or RNA molecules. Unlike traditional Sanger sequencing, which could only sequence a limited number of DNA fragments at a time, NGS can analyze millions of DNA fragments simultaneously in a single run. This massive parallel sequencing capacity has drastically increased the speed, efficiency, and scale of DNA sequencing, making it an indispensable tool in modern genomics and biomedical studies.</p> <p>NGS workflows encompass key steps, starting with sample preparation, where DNA or RNA is extracted and fragmented into smaller segments. Subsequently, unique identifiers are added to the fragments during library preparation, enabling multiplexed sequencing. The fragments are then amplified and sequenced in parallel using state-of-the-art NGS platforms. Finally, data analysis methods are applied to process the raw sequencing data, reconstruct the original DNA or RNA sequence, and identify genetic variations, structural changes, or functional elements.</p> <p>The versatility of NGS extends far beyond genome sequencing. It is widely utilized in various applications, including transcriptome analysis (RNA-Seq), epigenetic profiling (ChIP-Seq), metagenomics, and targeted sequencing. NGS has revolutionized fields like oncology, infectious disease research, and personalized medicine, as its ability to generate vast amounts of data rapidly provides unprecedented insights into the genetic basis of diseases and biological processes. As NGS technologies continue to advance and become more accessible, they will remain at the front of cutting-edge genomics research, driving innovations that contribute to our understanding of complex genetic interactions and their implications for human health and biology.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#data-produced-during-an-ngs-experiment","title":"Data produced during an NGS experiment","text":"<p>During an NGS experiment, various types of data are generated, including everything needed to create and prepare the samples used for sequencing. These compass, between others, laboratory protocols, lab notebooks, NGS libraries and the sequencing data itself.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#electronic-laboratory-notebook","title":"Electronic Laboratory Notebook","text":"<p>An electronic laboratory notebook (ELN) is a digital version of the traditional paper notebook used by scientists, researchers, and professionals in laboratories and research settings. It serves as a platform for recording, organizing, and managing experimental data, observations, protocols, and other research-related information in electronic format. ELNs offer a range of features including data entry, text editing, file attachments, collaboration tools, and data search capabilities. They provide advantages over paper notebooks by enabling easy data retrieval, backup, and sharing, as well as facilitating collaboration among researchers.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#laboratory-protocols","title":"Laboratory protocols","text":"<p>A laboratory protocol is documented set of procedures and methodologies outlining the precise steps required to prepare and manage samples of an experiment.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#samples","title":"Samples","text":"<p>In the context of molecular biology and Next-Generation Sequencing (NGS), a sample refers to a specific biological material or substance that is collected and prepared for genetic analysis. This material can encompass various biological entities such as DNA, RNA, or proteins, depending on the objectives of the study. For instance, a DNA sample might be extracted from a tissue or blood sample to analyze genetic variations or sequences. In NGS experiments, samples are typically processed according to specific protocols to generate large amounts of sequencing data, allowing researchers to delve into the genetic information contained within the sample.</p> <p>Bioinformatics data from your samples</p> <p>The experimental conditions of each of the samples should be registered in a metadata file that can be used later in your data analyses, such as treatment, cell type, timepoints, etc.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#library-preparation","title":"Library Preparation","text":"<p>After preparing your samples, the next step in an NGS experiment is the library preparation, where DNA or RNA samples are processed to create DNA fragments with specific adapter sequences. These adapters contain unique barcode sequences that allow the identification of individual samples within a multiplexed sequencing run.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#sequencing","title":"Sequencing","text":"<p>Once the libraries are prepared, they undergo the sequencing process using NGS platforms such as Illumina, Oxford Nanopore Technologies (ONT), or Pacific Biosciences (PacBio). The choice of platform depends on factors such as read length, error rate, throughput, and cost. The sequencing generates millions to billions of short DNA or RNA fragments, known as reads.</p> <p>The sequencing process produces raw data in the form of short reads, each representing a small segment of the genome or transcriptome. These reads are typically stored in FASTQ files, which contain nucleotide sequences and corresponding quality scores for each read.</p> <p>Bioinformatics data from sequencing</p> <ul> <li>FASTQ: This is one of the primary file formats used to store raw NGS data. FASTQ files contain nucleotide sequences and their corresponding quality scores. Each read from the sequencing process is represented as a set of four lines: a read identifier, the sequence of nucleotides, a separator (usually a '+'), and the quality scores for each base in the sequence.</li> </ul>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#data-produced-during-preprocessing","title":"Data produced during preprocessing","text":"","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#quality-control","title":"Quality Control","text":"<p>Before proceeding with data analysis, quality control is performed to assess the accuracy and reliability of the raw sequencing data. Low-quality reads and potential sequencing artifacts are filtered out to ensure the reliability of downstream analyses. Most useful tools to track Quality Control are FastQC and MultiQC.</p> <p>!!! \"Bioinformatics data from QC\"</p> <pre><code>- **FastQC**: FastQC is a widely-used bioinformatics tool that provides a comprehensive assessment of the quality of sequencing data, generating valuable insights into potential issues or biases in the data. \n- **MultiQC**: MultiQC is a companion tool that simplifies the process of aggregating and visualizing quality control metrics from multiple samples or datasets, allowing for a quick overview of data quality across an entire project.\n</code></pre>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#data-alignment","title":"Data Alignment","text":"<p>In DNA sequencing, the reads are aligned to a reference genome using alignment algorithms to determine the specific location of each read in the genome. For RNA-Seq experiments, reads are aligned to a reference transcriptome to identify gene expression levels.</p> <p>!!! \"Bioinformatics data from alignment\"</p> <pre><code>- **BAM/SAM**: These are binary and text versions, respectively, of Sequence Alignment/Map (SAM) files. SAM files store the alignment information of sequencing reads to a reference genome or transcriptome. BAM files are compressed and more space-efficient, making them the preferred format for storing aligned reads.\n- **FASTA**: FASTA files store nucleotide or amino acid sequences, and they are often used for reference sequences or assembled contigs. Each sequence in a FASTA file begins with a single-line description, followed by the actual sequence data.\n- **GTF/GFF**: These files are used to annotate genomic features, such as genes, exons, and transcripts. Gene Transfer Format (GTF) and General Feature Format (GFF) files include information on feature positions, names, and additional attributes.\n- **Alignment indexes**: These are reference data structures that facilitate efficient and rapid mapping of sequencing reads to a reference genome or transcriptome during Next Generation Sequencing (NGS) data analysis.\n</code></pre>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#variant-calling","title":"Variant Calling","text":"<p>For genomic DNA sequencing, variant calling identifies differences (mutations, insertions, deletions) between the sequenced sample and the reference genome. This step is crucial for detecting genetic variations associated with diseases or phenotypic traits.</p> <p>!!! \"Bioinformatics data from VC\"</p> <pre><code>- **VCF**: Variant Call Format (VCF) files store genetic variations, such as single nucleotide variants (SNVs), insertions, deletions, and structural variants, identified during variant calling. VCF files include variant position, alleles, genotype information, and quality scores.\n</code></pre>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#rna-expression-analysis","title":"RNA Expression Analysis","text":"<p>In RNA-Seq experiments, data analysis includes quantification of gene expression levels, detection of alternative splicing events, and identification of differentially expressed genes under different experimental conditions.</p> <p>!!! \"Bioinformatics data from Expression Analyses\"</p> <pre><code>- **Count matrix**: represents the quantified number of times each gene or genomic feature is observed in a set of biological samples. Each row typically corresponds to a gene, while each column represents a sample.\n</code></pre>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#epigenetic-profiling","title":"Epigenetic Profiling","text":"<p>Epigenetic analyses, such as ChIP-Seq, assess DNA modifications and protein-DNA interactions, providing insights into gene regulation and chromatin structure.</p> <p>!!! \"Bioinformatics data from Epigenetic profiling\"</p> <pre><code>- **Peak Calling Results**: For ChIP-Seq or other genomic profiling experiments, peak calling identifies regions with enriched signal intensity. The results are typically presented in BED or BEDGraph formats, indicating the genomic positions and signal intensities of detected peaks, or BigWig files for visualization in a genome browser.\n- **BED/BEDGraph**: These files are used to represent genomic intervals, such as gene coordinates, regions of interest, or coverage information. BED files define genomic regions with start and end coordinates, while BEDGraph files represent continuous data (e.g., coverage) as a graph.\n- **WIG/BigWig**: These files store genome-wide data, such as coverage, signal intensity, or ChIP-Seq peaks. Wiggle (WIG) files contain continuous data, while BigWig files are binary, compressed versions that enable efficient random access to large datasets.\n</code></pre>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#metagenomic-analysis","title":"Metagenomic Analysis","text":"<p>For metagenomics, the NGS data is used to characterize microbial communities in environmental samples, including the identification of species and functional genes. Overall, the data generated during an NGS experiment is extensive and diverse, providing a wealth of information that is crucial for a wide range of biological and medical research applications. The interpretation and analysis of this data require sophisticated bioinformatics tools and pipelines, as well as domain-specific knowledge to derive meaningful biological insights.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#other-types-of-data","title":"Other types of data","text":"<p>There are many more data formats we have not explored here. You are welcome to check more bioinformatics data formats at the UCSC webpage.</p> <p></p> <p>Bioinformatics data format tutorial</p> <p>Check out this tutorial for a more in-depth explanation of different bioinformatics data formats</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#data-produced-during-data-analysis","title":"Data produced during data analysis","text":"<p>Results from NGS data analysis often involve a variety of visualizations and tabular outputs that provide valuable insights into the biological significance of the data. Here are some common types of results that researchers may obtain from NGS data analysis.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#variant-annotation","title":"Variant Annotation","text":"<p>After variant calling, the identified genetic variations are annotated with functional information, such as their impact on genes, regulatory elements, or protein products.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#differential-expression-analysis","title":"Differential Expression Analysis","text":"<p>Differential Expression Analysis Tables: Differential expression analysis compares gene expression levels between different experimental conditions or groups. The results are typically presented in tabular format, listing genes with significant changes in expression, along with log-fold changes and statistical significance values.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#differential-binding-or-chromatin-accesibility-analysis","title":"Differential Binding or Chromatin Accesibility Analysis","text":"<p>Differential Binding Analysis Tables: Differential expression analysis compares chromatin accessibility or binding levels between different experimental conditions or groups. The results are typically presented in tabular format, listing genes with significant changes in binging or chromatin accesibility, along with log-fold changes and statistical significance values.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#functional-analysis","title":"Functional analysis","text":"<p>Functional analysis identifies functional categories, pathways, or gene sets that are significantly overrepresented in a list of differentially expressed genes. The results are presented as tables with enrichment scores and adjusted p-values, indicating the biological relevance of gene sets.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#metagenomic-analysis_1","title":"Metagenomic Analysis","text":"<p>Metagenomic Taxonomic Abundance Tables: In metagenomics, taxonomic abundance tables show the relative abundance of different microbial taxa across multiple samples. They are essential for understanding the composition of microbial communities.</p> <p>Metagenomic Functional Analysis Results: Metagenomic functional analysis identifies the functional capabilities of microbial communities. Results are typically presented as tables showing the relative abundance of functional genes or pathways.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#other-types-of-data_1","title":"Other types of data","text":"<p>These results from NGS data analysis play a crucial role in interpreting the biological significance of the data, enabling researchers to draw meaningful conclusions and generate hypotheses for further investigation. Effective data visualization and clear tabular outputs are essential for effectively communicating and interpreting the wealth of information generated by NGS experiments.</p> <p>Bioinformatics visualizations</p> <ul> <li>Heatmaps: Heatmaps are graphical representations of data matrices, where each cell's color intensity reflects the value of a specific parameter. In NGS data analysis, heatmaps are frequently used to visualize gene expression patterns, epigenetic modifications, or microbial abundances across different samples or conditions.</li> <li>Volcano Plots: Volcano plots display the relationship between fold change (usually log-fold change) and statistical significance (e.g., p-value) for each gene in a differential analysis. Genes with high significance and large fold changes are often represented further away from the plot's center.</li> <li>Genome Browser Snapshots: Genome browser snapshots display aligned sequencing reads and various genomic features (e.g., gene annotations, ChIP-Seq peaks) in a genomic region of interest. These snapshots provide a visual representation of NGS data aligned to a reference genome.</li> <li>Network Visualizations: Network visualizations depict the interactions between genes, proteins, or other biological entities, providing insights into complex biological networks. These visualizations can help uncover regulatory relationships or functional modules.</li> <li>Genomic Annotations: Results may include annotations for genetic variations, such as their functional impact on genes, genomic regions, or regulatory elements.</li> </ul>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#other-resources-used","title":"Other resources used","text":"","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#knowledge-databases","title":"Knowledge databases","text":"<p>A knowledge database is a structured repository of biological information that categorizes and annotates genes, proteins, and their functions, facilitating comprehensive understanding and analysis of biological systems. Here are five examples of knowledge databases:</p> <ol> <li>Gene Ontology (GO): A comprehensive resource that classifies gene functions into defined terms, allowing for standardized annotation and comparison of genes across different organisms.</li> <li>Disease Ontology: A database that provides structured, standardized terminology for various diseases and their relationships, aiding in the systematic analysis of disease-related data.</li> <li>KEGG Pathways: A collection of manually curated pathway maps representing molecular interactions and reaction networks within cells, enabling the interpretation of high-throughput data in the context of biological systems.</li> <li>Reactome: An open-access database that offers curated descriptions of biological processes, including pathways, reactions, and molecular events, facilitating the interpretation of large-scale biological data.</li> <li>UniProt: An extensive protein knowledgebase that provides detailed information about proteins, including their sequences, functions, and related annotations, supporting a wide range of biological research endeavors.</li> </ol>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#code-notebooks-and-scripts","title":"Code notebooks and scripts","text":"<p>Code notebooks and scripts are essential tools used to analyze Next Generation Sequencing (NGS) data, providing researchers with a documented and reproducible workflow for data processing, and analyses. Common programming languages (and notebooks) are: Python (Jupyter Notebooks), R (Rmarkdown), perl and bash.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#workflows-and-pipelines","title":"Workflows and pipelines","text":"<p>These are structured sets of interconnected data analysis steps and bioinformatics tools that streamline and automate the transformation of raw sequencing reads into biologically meaningful information, ensuring reproducibility and efficiency in NGS data analysis. Some examples of workflow and pipeline languages are:</p> <ul> <li>Nextflow: Nextflow is a popular workflow management system that enables scalable and portable NGS data analysis pipelines, allowing researchers to process data across various computing environments and platforms.</li> <li>Snakemake: Snakemake is a workflow management system that uses Python-based scripting to create flexible and automated NGS data analysis pipelines, facilitating parallel processing and easy integration with existing tools.</li> </ul> <p>A great example of community curated workflows is the nf-core community. Nf-core is a collaborative and open-source initiative comprising bioinformaticians and researchers dedicated to developing and maintaining a collection of curated and reproducible Nextflow-based pipelines for NGS data analysis, ensuring standardized and efficient data processing workflows.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#data-formats-summary","title":"Data formats summary","text":"<p>Here is a summary of different file formats used for NGS data and its results. It's important to select appropriate file formats that balance data accessibility, storage efficiency, and compatibility with downstream analysis tools. Additionally, the use of standardized file formats facilitates data sharing and collaboration among researchers in the scientific community.</p> <ul> <li>Tabular formats: File formats like CSV, TSV, and XLSX used to store data in rows and columns for easy data analysis and sharing.</li> <li>Image formats: File formats such as PNG and SVG used to store graphical visualizations, making them easily viewable and shareable.</li> <li>Binary formats: File formats like NPZ and H5 used to store large datasets, ensuring efficient data access and storage.</li> <li>BED: A format for storing genomic interval data, often used for representing genomic features and peak calling results.</li> <li>GTF: A format for storing genomic feature annotations, such as genes and transcripts, providing information on their positions and attributes.</li> <li>VCF: Variant Call Format used to store genetic variations and their attributes identified during variant calling.</li> <li>JSON: A lightweight data-interchange format for storing hierarchical data structures, commonly used in bioinformatics tools.</li> <li>HTML: A format used to create interactive reports that include both visualizations and textual descriptions of analysis results.</li> <li>FASTQ: A format for storing raw NGS data, containing nucleotide sequences and quality scores for each read.</li> <li>FASTA: A format for storing nucleotide or amino acid sequences, often used for reference sequences or assembled contigs.</li> <li>BAM/SAM: BAM is A binary format used to store compressed alignment data, representing aligned reads to a reference genome. SAM is a text version of the BAM format, representing aligned reads to a reference genome.</li> <li>Alignment indexes: Reference data structures facilitating efficient mapping of reads to a reference genome during NGS data analysis.</li> <li>FASTQC: A tool generating quality control reports for raw NGS data, assessing read quality and detecting potential issues.</li> <li>MultiQC: A tool aggregating results from multiple QC analyses, simplifying the overview and comparison of multiple datasets.</li> <li>Code notebooks: Interactive documents combining code, visualizations, and explanatory text, aiding in data analysis reproducibility and documentation.</li> <li>Scripts: Text files containing sets of commands or code instructions for automating data processing and analysis tasks.</li> </ul>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"02_NGS_data.html#wrap-up","title":"Wrap up","text":"<p>In this lesson we have taken a look a the vast and diverse landscape of bioinformatics data. This should give you a start in understanding the type of data we will be focusing on for the next lessons. In the next chapter we will talk about its life cycle, which will give you an idea about what do you need to pay attention in order to organize, document and share your data.</p>","tags":["NGS","Bioinformatics data","Data formats"]},{"location":"03_data_life_cycle.html","title":"Data Life Cycle","text":"","tags":["Data Life Cycle"]},{"location":"03_data_life_cycle.html#data-life-cycle","title":"Data Life Cycle","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Understand what the Data Life Cycle is</li> <li>Understand what each of the phases of the Data Life Cycle.</li> </ol> <p>The data life cycle is a comprehensive and systematic approach that encompasses the entire journey of research data, from its inception through its collection, processing, analysis, sharing, and preservation. Like a living organism, data goes through different stages in its life cycle, each with specific considerations and requirements. Understanding and managing the data life cycle is crucial for researchers to ensure data integrity, accessibility, and long-term usability. By following the data life cycle, researchers can effectively organize, document, and share their data, fostering transparency, reproducibility, and collaboration in scientific research.</p>","tags":["Data Life Cycle"]},{"location":"03_data_life_cycle.html#data-life-cycle-phases","title":"Data Life Cycle phases","text":"<p>UCPH describes the data life cycle in 6 phases:</p> <ol> <li>Plan: In the planning stage, researchers define the objectives of their data collection and analysis, identify data requirements, and develop a data management plan. This plan outlines how data will be collected, stored, and shared, as well as ethical and legal considerations.</li> <li>Collect and Document: In this stage, researchers gather the data according to their plan. They document important details about the data, such as its source, collection methods, and any modifications made during data acquisition. Proper documentation ensures data quality and facilitates later use.</li> <li>Process and Analyse: Data is processed and analyzed to draw meaningful insights and conclusions. Researchers use various methods, tools, and algorithms to extract valuable information from the collected data. During this phase, data is transformed, cleaned, and transformed into usable formats.</li> <li>Store and Secure: Storing data securely is essential to protect it from loss, unauthorized access, and corruption. Researchers must choose appropriate storage solutions and implement data security measures to safeguard sensitive or confidential information.</li> <li>Share: Sharing data is an important aspect of the data life cycle. Researchers are encouraged to share their data openly whenever possible, adhering to Open Science and FAIR principles. Data sharing fosters collaboration, increases the visibility of research, and enables data reuse by others.</li> <li>Preserve: Data preservation ensures that valuable research data is available and usable in the long term. Researchers should deposit data in trusted data repositories or archives, ensuring its ongoing accessibility and future use by the scientific community.</li> </ol> <p> Research Data Life Cycle, University of Copenhagen RDM guidelines. </p> <p>Let's take a look at each of the phases:</p>","tags":["Data Life Cycle"]},{"location":"03_data_life_cycle.html#plan","title":"Plan","text":"<p>In the planning stage, researchers define the objectives of their data collection and analysis, identify data requirements, and develop a data management plan. This plan outlines how data will be collected, stored, and shared, as well as ethical and legal considerations.</p> <p>The management of research data must be thoroughly considered before physical materials (samples, model organisms, reagents or media, etc) and digital data (bioinformatics data) are collected, observed, generated, created or reused. Data management plans (DMP) must be developed and documented, preferably in electronic format. DMPs should be updated when significant changes to the management of research data occur and (references to) the DMP should be stored with the corresponding research data as long as they exist.</p> <p>Warning</p> <p>This is a reminder that this course focuses only on bioinformatics digital data, not physical samples or materials.</p> <p>The DMP should be discussed with project collaborators, research managers and supervisors (if any), ensuring that agreements are reached regarding responsibilities for different research data management activities during and after research projects.</p> <p>Tip</p> <p>We will check about how to write a DMP for NGS data in the 5<sup>th</sup> lesson.</p>","tags":["Data Life Cycle"]},{"location":"03_data_life_cycle.html#collect-and-document","title":"Collect and Document","text":"<p>In this stage, researchers gather the data according to their plan. They document important details about the data, such as its source, collection methods, and any modifications made during data acquisition. Proper documentation ensures data quality and facilitates later use.</p> <p>Research data should be collected and processed in line with best practice in the research discipline. For example, there will be important differences between a metagenomics project and a clinical trial involving human samples! Research projects should be documented in a way that allows them to be repeated by others. Among other things, this includes clearly and accurately describing project methodology and any equipment, software or code used. This includes workflows for data preprocessing and how you will structure and organize your files.</p> <p>Tip</p> <p>We will see about how to structure your files and folders in the 6<sup>th</sup> lesson.</p> <p>In addition, research data should be described using appropriate metadata to facilitate searching for, the identification of, and the interpretation of the research data. Metadata should be linked to the research data as long as they exist, unless legislation or agreements state otherwise.</p> <p>Tip</p> <p>We will see about what kind of metadata you can use for your NGS data in the 7<sup>th</sup> lesson</p>","tags":["Data Life Cycle"]},{"location":"03_data_life_cycle.html#process-and-analyse","title":"Process and analyse","text":"<p>This, in the data life cycle for NGS data, is a critical phase that involves transforming raw sequencing data into meaningful biological insights. During this stage, researchers apply computational methods and bioinformatics tools to extract valuable information from the vast amount of sequencing data generated in NGS experiments.</p> <p>Throughout this phase, researchers should adhere to good coding practices, ensuring well-documented and reproducible analyses. Code notebooks and version control tools, such as Git, help maintain transparency and facilitate the sharing of methods and results with the scientific community.</p> <p>To streamline and standardize the data analysis process, researchers often employ workflows and pipelines. Workflows automate the execution of multiple analysis steps, enhancing efficiency and consistency while promoting reproducibility. The NGS research community benefits from collaborative efforts, such as the nf-core community, which provides pre-established and validated Nextflow-based pipelines for various NGS applications. Leveraging community-developed pipelines ensures adherence to best practices and accelerates the pace of research through shared expertise.</p> <p>Tip</p> <p>We will see more about version control in lesson 9</p>","tags":["Data Life Cycle"]},{"location":"03_data_life_cycle.html#store-and-secure","title":"Store and Secure","text":"<p>Storing data securely is essential to protect it from loss, unauthorized access, and corruption. Researchers must choose appropriate storage solutions and implement data security measures to safeguard sensitive or confidential information.</p> <p>Research data must be classified at the start of a research project on the basis of the level of sensitivity and the impact to the University if data are disclosed, altered or destroyed without authorisation. Risks to data security and of data loss should be assessed in relation to the data classification. This includes evaluating:</p> <ul> <li>Physical and digital access to research data</li> <li>Risks associated with data management procedures</li> <li>Backup requirements and backup procedures</li> <li>External and internal threats to data confidentiality, integrity and accessibility</li> <li>Financial, regulatory and technical consequences of working with data, data storage and data preservation</li> </ul> <p>Warning</p> <p>This step is very specific to the setup used in your environment. Maybe you are using a department server with storage, or you use a personal drive, or cloud solutions such as AWS. Thus, we cannot include in this course a comprehensive guideline on this matter.</p> <p>About GPDR and protected data</p> <p>This course does not talk in detail about protected NGS data, such as human or patient samples. For more information about GPDR protected data, check out our course!</p>","tags":["Data Life Cycle"]},{"location":"03_data_life_cycle.html#sharing","title":"Sharing","text":"<p>Sharing data is an important aspect of the data life cycle. Researchers are encouraged to share their data openly whenever possible, adhering to Open Science and FAIR principles. Data sharing fosters collaboration, increases the visibility of research, and enables data reuse by others.</p> <p>Legislation or agreements may preclude research data sharing or impose conditions for sharing. Before sharing research data, the relevant approvals need to be obtained and, if necessary, the appropriate agreements set up to allow data and material sharing.</p> <p>By default, research data should be made openly available after project end, as a minimum for data sets underlying research publications. Concerns relating to intellectual property rights, personal data protection, information security as well as commercial and national interests and legislation must be taken into account in accordance with the principle of \u2018as open as possible, as closed as necessary\u2019. If the research data cannot be made available, sharing the metadata associated with the research data should be considered. The FAIR principles (for findable, accessible, interoperable and reusable research objects) should be followed as much as possible when preparing digital data sets that can be shared. This includes as a minimum:</p> <ul> <li>Providing open access to data (Open Data) by depositing data in a data repository, or by providing access to information on whether, when, how, and to what extent data can be accessed if data sets cannot be made openly available.</li> <li>As much as possible using persistent identifiers (PID) and metadata (such as descriptive keywords) that help locate the data set.</li> <li>Communicating terms and conditions for data reuse by others, for example by attaching a data licence.</li> <li>Providing the information necessary to understand how data sets were created and structured, and for what purpose.  </li> </ul> <p>Tip</p> <p>We will talk about FAIR and OS principles in the next lesson</p>","tags":["Data Life Cycle"]},{"location":"03_data_life_cycle.html#preserve","title":"Preserve","text":"<p>Data preservation ensures that valuable research data is available and usable in the long term. Researchers should deposit data in trusted data repositories or archives, ensuring its ongoing accessibility and future use by the scientific community.</p> <p>Appropriate arrangements for the long-term preservation of digital data, physical material and associated metadata must be made, adhering to legislation and/or agreements. This should include:</p> <ul> <li>Deciding which research data will be preserved. As a minimum, data sets underlying published research results must be preserved so that any objections or criticisms can be addressed.</li> <li>Deciding how long research data will be preserved. Data sets underlying research publications should be retained for at least five years after project completion or date of publication, whichever comes last.</li> <li>Choosing a format and location in which research data should be preserved, and deciding what metadata should be associated with the preserved data and material.</li> <li>Deleting/destructing research data if legislation or agreements exclude preservation, or when researchers and their managers determine that preservation is not required (for example when research data can easily be reproduced) or not possible (for example when research data are too costly to store or when material quality will deteriorate over time).</li> <li>Assigning a person, persons or role(s) responsible for the research data after project end. Responsibilities include safeguarding the long-term integrity of data sets.</li> <li>Determining rights, for example, of access to and use of preserved data sets.</li> </ul> <p>In addition, your institution may oblige you to preserve and keep a copy of your data at their premises. They will usually explain any requirement at their own data management department. For example, the UCPH mandates that a copy of data sets and associated metadata must remain at UCPH after project end and/or when employment with the University ceases, in a way in which they are accessible to research managers and understandable for research managers and peers, unless legislation or agreements determine otherwise.</p> <p>Tip</p> <p>We will check about which repositories you can use to preserve your NGS data in the 10<sup>th</sup> lesson</p>","tags":["Data Life Cycle"]},{"location":"03_data_life_cycle.html#wrap-up","title":"Wrap up","text":"<p>In this lesson we have learned about the Research Data Life Cycle, from the conception of a project and collection of data until the end of the project and preservation of the data. These concepts were taken from the UCPH data management guidelines and put them in the context of NGS, but are perfectly usable in other universities and applications.</p>","tags":["Data Life Cycle"]},{"location":"04_OS_FAIR.html","title":"Introduction to Open Science and FAIR principles","text":""},{"location":"04_OS_FAIR.html#introduction-to-open-science-and-fair-principles","title":"Introduction to Open Science and FAIR principles","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Learn about Open Science</li> <li>Learn about FAIR principles</li> <li>Learn about how they can be applied to NGS data  </li> </ol> <p>Open Science and FAIR principles have emerged as powerful frameworks that promote transparency, accessibility, and reusability in scientific research. As the landscape of scientific knowledge sharing evolves, these principles stand at the front of a global movement to accelerate scientific discovery and innovation. Open Science advocates for the unrestricted access to research outputs, data, and methodologies, fostering collaboration and opening scientific knowledge. On the other hand, the FAIR principles emphasize the importance of making data Findable, Accessible, Interoperable, and Reusable, laying the foundation for a data-driven approach that transcends disciplinary boundaries and supports long-term data preservation. Together, Open Science and FAIR principles offer a vision for a more inclusive and impactful scientific ecosystem that advances the boundaries of knowledge for the benefit of society as a whole. Funding agencies and governments worldwide are increasingly recognizing the value and potential of Open Science and FAIR principles in advancing research and driving societal impact. As a result, they have been actively promoting and requiring the adoption of these principles in academia through various mechanisms and policies. In this lesson we will learn what is Open Science and the FAIR principles, and how they can be applied to NGS data.</p>"},{"location":"04_OS_FAIR.html#open-science","title":"Open Science","text":"<p>Funding agencies and regulatory agencies, such as the EU, have been actively promoting Open Science through various policies, recognizing its potential to advance research, innovation, and societal well-being. The reasons for this push are plentiful. Open Science allows research outputs to be accessible to a wider audience, accelerating the impact of research and fostering collaboration among researchers and institutions, as well as encourages transparency and reproducibility, fostering responsible research conduct and mitigating issues like scientific fraud and data manipulation. The reasons are also economical: by promoting Open Access and data sharing, the EU seeks to maximize the return on its research investments by ensuring that knowledge and data are used and built upon by the broader scientific community. This also provides a foundation for innovation and entrepreneurship, enabling businesses and startups to access and leverage research findings for creating new products and services, and allows researchers to integrate and analyze large datasets from different sources, leading to new insights and discoveries.</p> <p> Open Science, Australian Citizen Science Association. </p> <p>More and more agencies are promoting Open Science initiatives, supporting research that adheres to OS principles. Here are some more examples:</p> <ul> <li>National Institutes of Health (NIH): In the United States, NIH encourages Open Science practices, such as data sharing, through various policies and initiatives. The NIH Data Sharing Policy requires researchers to share their data with the scientific community.</li> <li>Wellcome Trust: The Wellcome Trust, a global charitable foundation, has a strong commitment to Open Science. It mandates that research outputs funded by Wellcome must be made openly available through Open Access platforms.</li> <li>European Molecular Biology Organization (EMBO): EMBO supports Open Access to research outputs and provides guidelines for best practices in data sharing.</li> <li>Bill &amp; Melinda Gates Foundation: The foundation advocates for Open Access and data sharing to maximize the impact of its global health and development research.</li> <li>European Research Council (ERC): The ERC promotes Open Access to research outputs, requiring grantees to make their publications openly accessible. It also encourages data sharing and adheres to the FAIR principles to ensure the findability, accessibility, interoperability, and reusability of research data.</li> </ul> <p>Benefits of Open Science</p> <p>While it may seem that this push for Open Science is being enforced by funding agencies to get more bang for their buck, adhering to Open Science practices can offer numerous benefits to you, as a researcher. Some of the key advantages include:</p> <ul> <li>Increased Visibility and Impact: When you share your research openly, more people can access and read your papers and data, increasing the visibility and impact of your work.</li> <li>Enhanced Collaboration: Open Science encourages collaboration with other researchers, leading to new ideas and projects that can be more impactful.</li> <li>Build Trust in Your Findings: By sharing your data and methods openly, other researchers can verify and validate your findings, increasing the credibility of your research.</li> <li>Research Moves Faster: Open Science lets researchers build upon each other's work, accelerating the pace of discovery and progress in science.</li> <li>Inspire New Discoveries: Your shared data can inspire others to explore new research questions and find insights you might not have considered.</li> <li>Attract Funding Opportunities: More and more funding agencies are supporting Open Science, and following open practices can help you qualify for more funding opportunities.</li> <li>Transparency and Accountability: Open Science practices promote transparency, accountability, and responsible conduct in research, reducing the risk of scientific misconduct.</li> <li>Preserve Your Data: By archiving your data in repositories, you ensure that it is preserved and available for future generations of scientists.</li> </ul>"},{"location":"04_OS_FAIR.html#fair-principles","title":"FAIR principles","text":"<p>The FAIR principles are a set of guiding principles designed to enhance the management, sharing, and usability of research data, and could be considered as complementary to Open Science. FAIR stands for Findable, Accessible, Interoperable, and Reusable, and these principles aim to make research data more valuable, impactful, and sustainable. By adhering to the FAIR principles, researchers and institutions can maximize the value and impact of their research data. FAIR data not only benefits individual researchers but also contributes to the broader scientific community by fostering collaboration, data-driven discoveries, and the advancement of knowledge across disciplines. Furthermore, FAIR data supports long-term data preservation, making it valuable for future research and ensuring the sustainability of scientific progress.</p> <p>Take note that adhering to the FAIR principles is not a black and white matter. There are different measure of FAIR compliance and some key points are more complicated to accomplish than others. This is specially true when we talk about using metadata standards and controlled vocabularies!</p> <p>Let's break down each FAIR principle:</p>"},{"location":"04_OS_FAIR.html#findable","title":"Findable","text":"<p>Research data should be easy to find and identify. To achieve this, data should be assigned persistent and unique identifiers (e.g., DOIs) and described using standard metadata. Providing clear and comprehensive metadata helps researchers and machines discover relevant data through various search engines and data repositories.</p> <p>Key components</p> <ul> <li>Persistent Identifiers: Research data should be assigned unique and persistent identifiers, such as Digital Object Identifiers (DOIs) or Uniform Resource Identifiers (URIs). These identifiers ensure that data can be located and referenced over time, even if it is moved or its location changes.</li> <li>Rich Metadata: Data should be accompanied by comprehensive and standardized metadata that describes the content, context, and characteristics of the data. This metadata includes information about the data's origin, format, version, licensing, and the conditions for reuse.</li> <li>Data Repository or Catalog: Data should be deposited in trusted repositories or data catalogs that follow FAIR principles, making it easier to discover and access data from a centralized and reliable source.</li> </ul>"},{"location":"04_OS_FAIR.html#accessible","title":"Accessible","text":"<p>Data should be openly accessible to anyone who needs it. This means that there should be minimal restrictions on accessing and downloading the data. Open access to data encourages collaboration, enables verification of research findings, and ensures transparency in research processes.</p> <p>Key components</p> <ul> <li>Open Access: Data should be openly accessible to anyone without restrictions or unnecessary barriers. Researchers should choose appropriate licenses or waivers that allow the broadest possible reuse of the data.</li> <li>Authentication and Authorization: If access restrictions are necessary (e.g., for sensitive or confidential data), proper authentication and authorization mechanisms should be in place to grant access only to authorized individuals or groups.</li> <li>Metadata: Always deposit the metadata for your data. Even if you cannot deposit your data due to restrictions, it is a good idea to deposit the metadata of your data so that at least the record for that data exists, and what kind of data/information contains.</li> </ul> <p>Important note</p> <p>Accessible does not equal to free, unrestricted access! There are cases when your data might be protected by GDPR regulations due to its sensitivity! If this is the case, you should, of course not make your data freely available. As said before, it is a great idea to at least deposit and make available the metadata for your data. </p> <p>\"As open as possible, as closed as necessary\" should be your motto.</p> <p>For example, imagine that you obtain your sensitive data from a national authority regarding personal information. Most likely, you will not be able to access all the data, but you will be interested in a sample of the population that fits your purposes (e.g., people between 20-30 years old, smokers). You cannot publish or deposit your data, but you can publish what was the query you used to obtain this data, and its source!</p>"},{"location":"04_OS_FAIR.html#interoperable","title":"Interoperable","text":"<p>Data should be structured and formatted in a way that allows it to be used and combined with other data seamlessly. Following standard data formats and adopting widely used vocabularies and ontologies promotes interoperability, enabling integration and comparison of data from different sources.</p> <p>Key components</p> <ul> <li>Standard Data Formats: Data should be stored in standard and widely-used data formats, facilitating data exchange and interoperability with various software tools and platforms.</li> <li>Vocabularies and Ontologies: Researchers should use community-accepted vocabularies and ontologies to provide common definitions and concepts, ensuring data can be understood and combined with other datasets more effectively.</li> <li>Linked Data: By linking data with other related data and resources, researchers can enrich the context of their datasets, enabling better integration and discovery of interconnected information.</li> </ul>"},{"location":"04_OS_FAIR.html#reusable","title":"Reusable","text":"<p>Data should be well-documented and prepared for reuse in different contexts. This involves providing detailed information on data collection, processing, and methodology, allowing other researchers to understand and replicate the study. Additionally, licensing and ethical considerations should be clearly stated to enable legal and ethical reuse of the data.</p> <p>Key components</p> <ul> <li>Documentation and Provenance: Data should be accompanied by comprehensive documentation that explains how the data was collected, processed, and analyzed. Provenance information ensures that data users can understand the data's origin and processing history.</li> <li>Ethical and Legal Considerations: Researchers should provide clear information about ethical considerations related to data collection and use. Additionally, data should adhere to legal and regulatory requirements, ensuring its responsible and ethical reuse.</li> <li>Data Licensing: Researchers should clearly state the licensing terms for data reuse, specifying how others can use, modify, and redistribute the data while respecting intellectual property rights and legal constraints.</li> </ul>"},{"location":"04_OS_FAIR.html#open-science-and-fair-principles-applied-to-ngs-data","title":"Open Science and FAIR principles applied to NGS data","text":"<p>In this section we will see how we can apply Open Science and FAIR principles to your NGS data. Note that not all data needs to be archived and deposited. NGS data processing generates vast amounts of data that might not need to be share publicly, as long as you describe how you produced the data. For example, in an usual bulk RNAseq experiment, FASTQ reads are cleaned and subsequently aligned to a reference genome, creating a subset of cleaned FASTQ files and BAM files. After transcript/gene quantification, you can obtain a final count matrix that can be used for data analysis, such as Differential Expression Analysis. If you provide enough documentation on how these files were processed (which softwares, which versions and which options), you won't need to deposit neither the cleaned nor aligned reads, only the original FASTQ files and the final result of your preprocessing. This will save quite the computational resources and metadata needed to preserve the intermediary data. Providing documentation on how the data was generated is much simpler if you are using community curated pipelines such as the ones created by the nf-core community.</p> <p>You should share at least this data</p> <ul> <li>Raw NGS files in fastq format.</li> <li>Final preprocessing results, such as count matrices for RNAseq, peak calling results for ChIPseq/ATACseq, VCF files for variant calling, etc.</li> <li>Metadata about the raw files and final results.</li> </ul> <p>We will see what metadata you should submit below.</p>"},{"location":"04_OS_FAIR.html#open-science_1","title":"Open Science","text":"<p>Unless your data is of sensitive nature (human individual samples, patient data, or anything protected), you should always deposit your data with as little restrictions as possible. This includes publishing your manuscript in Open Access journals as well! For your generated NGS data, our suggestion is that you use a license such as Creative Commons licence like CC-BY license, which only requires users to attribute the source of the data, but this also depends on the repository that you use for your data. We will see more about it in the lesson 10</p> <p></p>"},{"location":"04_OS_FAIR.html#fair-principles_1","title":"FAIR principles","text":"<p>Next we will see how we can apply each of the FAIR principles to your NGS data.</p>"},{"location":"04_OS_FAIR.html#findable_1","title":"Findable","text":"<p>To make your NGS data easy to find, you should deposit it in a domain specific repository, such as the Gene Expression Omnibus or Annotare. We will see more about them in lesson 10. Both of these repositories will help you give your data a unique identifier, and provide information (metadata) on how the data was generated. The metadata you include in your submission should contain, at least, the minimum necessary information to understand what kind of data it is submitted, and how it was generated. This includes:</p> <ul> <li>Sample metadata in tabular format, containing information about the samples used in the experiment as well as variables of interest for the analysis.</li> <li>Experiment metadata, including data provenance, that is, how the samples were obtain, from which organism, following what protocols, kits, sequencing libraries, sequencing method and data preprocessing workflows/pipelines. This is usually submitted as part of a submission form and it depends on the repository.</li> <li>Keywords, such as type of NGS data, conditions or diseases studied in the experiment, organisms used, genes studied, etc.</li> </ul>"},{"location":"04_OS_FAIR.html#accessible_1","title":"Accessible","text":"<p>Both GEO and Annotare repositories promote the use of unrestricted access to the data. In the case of Annotare, deposited data is under CC0 license, while GEO states deposited data is public domain. Depositing your data will require you to have an account but it does not require authentication from the user to access and download the data.</p> <p></p> <p>On sensitive data</p> <p>If you would like to deposit sensitive data that needs controlled access, it is possible to do so through the European Genome-phenome Archive (EGA).</p> <p>In addition, if you have deposited your data with rich metadata, as explained in the previous step, it will be easier for users to query your data by date, author, organism, type of NGS data, etc etc.</p>"},{"location":"04_OS_FAIR.html#interoperable_1","title":"Interoperable","text":"<p>By using standard bioinformatics formats, such as fastq files for raw NGS data, count matrices in tabular format, BED files for peak calling results, etc., you are already complying to this section. In addition, GEO and Annotare repositories are complaint to NGS data standards, such as MIAME/MINSEQE/MINSCE guidelines.</p> <p></p> <p>Nonetheless, this is the easy part! Adhering to controlled vocabularies seems to be the most difficult part of the FAIR principles. Here are some cases:</p> <ul> <li>Using organism names instead of their taxonomy. For example: mouse instead of Mus musculus, or human, instead of Homo sapiens. Even better, we should use a taxonomy ID, such as the NCBI taxonomy ID for human NCBITaxon_9606, which will unequivocally refer to human.</li> <li>Using gene names or symbols instead of gene IDs. For example: the gene POU5F1 has many synonyms, like OCT4, OCT, OTF4. In order to be explicit, it is better to reference the gene ID, like an ENSEMBL gene ID ENSG00000204531.</li> <li>Using disease names instead of disease IDs. Again, this will reference specifically the disease you mention.</li> </ul> <p>There are many more stances where you can use controlled vocabularies for other variables of interest, like cell type, tissue, cell cycle, etc. We will see in the metadata lesson where you can find controlled vocabularies for different variables of interest in NGS data.</p>"},{"location":"04_OS_FAIR.html#reusable_1","title":"Reusable","text":"<p>In order for your NGS data to be reusable, you will have to provide a thorough documentation on how it was generated, as well as the terms (that is a license) on how the data can be used/retrieved. We have talked already about collecting metadata on how the samples were generated (laboratory protocols, sequencing library, kits, technology, etc) and processed (workflows or pipelines along with the software used, which versions adn options). We also talked on what type of standard file formats you should use, such as fastq files for raw data and tabular formats for sample metadata. Finally, we have discussed in the Open Science section that you should try to license your data as freely as possible, like a CC0 license or CC-BY license. If your data is of sensitive nature and has restricted access or conditions, you should instead provide information on how other people can access the data, as well as any agreements or ethical approvals necessary for its reuse.</p> <p></p>"},{"location":"04_OS_FAIR.html#wrap-up","title":"Wrap up","text":"<p>In this lesson we have learned about Open Science and the FAIR principles and how to apply them in NGS data. In the next lessons, we will see how to specifically apply this knowledge, from organising your data, what  metadata to collect and how to share it.</p>"},{"location":"05_DMP.html","title":"Data Management Plans","text":"","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#data-management-plans","title":"Data Management Plans","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Learn what is a DMP</li> <li>Learn about the different DMP templates</li> <li>How to write a DMP focused on NGS data</li> </ol> <p>There are a lot of terms regarding data management and a lot of best practices to collect and implement, but how do we gather all the decisions made and how do we know that we have covered it all, that our data will be well managed throughout its life cycle? The answer is to write a data management plan (DMP).</p> <p>A DMP is a document addressing requirements and practices for managing the project's data, code and documentation, throughout the data life cycle, i.e from the initial planning until the project ends and beyond. It outlines the data management strategies in a project. Making plans for how you will collect, document, organize, and preserve your data are all part of the data management strategy.</p>","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#why-are-dmps-important","title":"Why are DMPs important","text":"<p>There are several reasons why writing a data management plan is a very good idea:</p> <ul> <li>Think of the DMP as a checklist. Going through the checklist allows you to identify gaps in current data management strategies. Identifying the gaps early on saves a lot of headache and time spent later. Going through the process of planning is more important than the actual plan itself.</li> <li>In a project with several members, it is important to decide on standards that all collaborators should adhere to, e.g. regarding how to organise the data, how to name it, which metadata standards to use, what vocabularies to use, etc.</li> <li>Writing a DMP also enables you to estimate costs regarding data production, storage, data management, etc.</li> <li>It is also a good way to clarify responsibilities regarding the data and the data management, e.g. who is responsible for the execution of the DMP.</li> <li>By planning how the data will be managed, there's greater chance that the research data will be well-managed (no guarantee, since you still need to have good strategies and actually implement them for this to happen). Of course there are many benefits with well-managed data but the main ones are:<ul> <li>reproducibility, so that the results can be verified</li> <li>reusability, so that this data can be used for answering other scientific questions, thus reducing redundancy</li> <li>A DMP is the first step towards being FAIR in your project.</li> </ul> </li> </ul> <p>If the reasons above don't persuade you, the last argument is that it is becoming more and more a requirement by funders and other stakeholders:</p> <ul> <li>For transparency and openness: publicly funded research data must be discoverable, accessible, and reusable to the public</li> <li>Return on investment: well planned data maximizes the research potential of the data and provides greater returns on public investments and research.</li> </ul>","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#when-to-write-a-dmp","title":"When to write a DMP","text":"<p>A DMP is a living document, the initial version is written the same time as a new project idea is emerging, before e.g. applying for funds, and then successively updated as the project continues and new decisions are made. Ideally it should be updated continuously, but there are three major time points:</p> <ol> <li>Project planning: The DMP should outline the strategies for data management in sufficient detail to be able to estimate the resources needed to implement the DMP, so that this can be included in the proposal for funding (e.g. data production, data analysis, storage during and after project, costs related to publishing of data).</li> <li>Project start: The DMP is completed with more details e.g. about documentation, data quality measures, file and folder strategies, etc.</li> <li>Project end: The DMP is updated a final time with e.g. links to published data and details about archiving (what data and where), so that this document enables future re-use of the project (by yourself or others).</li> </ol>","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#the-main-parts-of-a-dmp","title":"The main parts of a DMP","text":"<ol> <li>Description of data<ul> <li>What types of data will be created and/or collected, in terms of data format and amount/volume of data?</li> </ul> </li> <li>Documentation<ul> <li>How will the material be documented and described, with associated metadata relating to structure, standards and format for descriptions of the content, collection method, etc.?</li> </ul> </li> <li>Storage and backup<ul> <li>How is data security, storage and backup of data and metadata safeguarded during the research process?</li> </ul> </li> <li>Legal and ethical aspects<ul> <li>How is data handling according to legal requirements safeguarded, e.g. in terms of handling of personal data, confidentiality and intellectual property rights?</li> </ul> </li> <li>Accessibility and long-term storage<ul> <li>How, when and where will research data or information about data (i.e. metadata) be made accessible? E.g. via deposition to international public repositories.</li> <li>In what way is long-term storage safeguarded, and by whom?</li> </ul> </li> <li>Responsibility and resources<ul> <li>Who are the responsible persons for data management?</li> <li>What resources (costs, labour input or other) will be required for data management?</li> </ul> </li> </ol>","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#dmp-templates","title":"DMP templates","text":"<p>Different agencies and funders may have different DMP templates that are mandatory for researchers to comply with data management requirements. Here you can find some examples.</p>","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#ucph-template","title":"UCPH template","text":"<p>The University of Copenhagen has developed a DMP template in alignment with the UCPH Policy on Research Data Manage\u200bment. This is the recommended template for all researchers, guests and students involved in research activities at UCPH, unless other specific DMP requirements from a funding agency or a partner institution apply.</p> <p>The UCPH DMP template is available within the DMPonline tool (see information below) or can be downloaded as a Word document.</p> <p>Guidance for all questions in this template can be found here.</p>","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#horizon-europe-and-erc-templates","title":"Horizon Europe and ERC templates","text":"<p>All projects funded in the Horizon Europe framework programme must prepare and submit a data management plan as deliverable according to the details described in the respective grant agreement.</p>","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#writing-a-dmp","title":"Writing a DMP","text":"<p>Standard DMP templates can typically be found at funder agencies, e.g. Swedish Research Council and Science Europe, and it is of course possible to write in your favorite text editor. In addition, there are several tools that can assist you in writing a DMP as well as guidance.</p> <p>Tools to write your DMP</p> <ul> <li>DMPonline: DMPOnline is an online tool for writing, sharing and reviewing data management plans. A Danish installation is available under dmponline.deic.dk. It is provided by the Danish e-Infrastructure Cooperation (DeiC) and jointly administered by the Royal Danish Library and DTU Library. DMPonline contains a number of different DMP templates, guidance texts and examples from Danish research institutions and relevant funders.  <ul> <li>The tool most universities have chosen to offer (check with your institute)</li> <li>Good guidance but typically generic and not Life Science specific</li> <li>Most often free text answers</li> <li>Contains guidance for several DMP templates</li> </ul> </li> <li>Data Stewardship wizard. DSW is a interactive tool to create data management plans based on questionnaires. <ul> <li>Provided by SciLifeLab</li> <li>Gives Life Science specific guidance</li> <li>Less free text answers, instead many questions with answer options</li> </ul> </li> </ul>","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#example-of-a-dmp-for-ngs-data","title":"Example of a DMP for NGS data","text":"<p>We are have written a DMP template that it is prefilled with repetitive information using DMPonline and the Horizon Europe guidelines. This template contains all the necessary information regarding common practices that we will use, the repositories we use for NGS, etc. The template is part of the <code>project</code> folder template, under <code>documents</code>. You can check the file here.</p> <p>The Horizon Europe template is mostly focused on digital data and so, it is maybe not the best option if you are also interested in recording in your DMP physical data, such as samples, reagents, media, cultures, model organisms, etc.</p> <p>Exercise: write a draft for a DMP</p> <p>You can write a DMP draft from scratch, or you could use the template mentioned above and modify it to your own needs!</p>","tags":["Data Management Plan","DMP"]},{"location":"05_DMP.html#wrap-up","title":"Wrap up","text":"<p>In this lesson we have learned about DMPs and tools that could help you write one, as well as shown you a possible template for your own projects! DMPs are a great tool to help you think about your project, the data you will reuse/generate, and how to preserve it properly for future use, either by you, collaborators or the broad scientific community. In the next lesson we will explore how you could organize your NGS data using a simple but clear system, and provide you some tools that will make the job easier.</p> <p>Some parts of this lesson are taken from the NBISweden workshop on RDM practices.</p>","tags":["Data Management Plan","DMP"]},{"location":"06_file_structure.html","title":"File structure and naming conventions","text":"","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#file-structure-and-naming-conventions","title":"File structure and naming conventions","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Learn how to organize your NGS data.</li> <li>Separate your data into projects and assays.</li> <li>Manage genomic resources.</li> <li>Learn general rules for naming files and folders.</li> <li>Create your own rules for naming specific results and figures.</li> </ol> <p>We have learn so far how to adhere to FAIR and Open Science standards. You may have noticed that they mostly cover information regarding sharing your data after your project is finished or your data is published. However, how can you effectively manage your data while you work with it? How should I organize my data folders, raw data, processed data, data analysis scripts, figures and results? You may not worry too much about it since you will be working on your data all the time, so you will know where everything is. However, projects tend to last long, and the data, its analysis and results, accumulate greatly over time. Not having any sort of structure will incur in many problems in the future, specially once the project is finished or some colleague or collaborator want to access the data. How will they be able to find X or Y table? Will you be able to remember where a figure came from or which script created a csv file? By not applying an efficient and clear system, you are inviting chaos to your doorstep!</p> <p>On the other hand, applying a consistent file structure and naming conventions to your files will help you to efficiently manage your data. Consider the following practices:</p> <ul> <li>Folder structure: Aim to establish a logical and intuitive folder structure that reflects the organization of research projects and experimental data. Use descriptive folder names to make it easy to locate and access specific data files.<ul> <li>Subfolders: Use subfolders to further categorize data based on their contents, such as code notebooks, results, reports, etc. This helps to keep data organized and facilitates quick retrieval.</li> </ul> </li> <li>File naming conventions: implement a standardized file naming convention to ensure consistency and clarity. Use descriptive names that include relevant information, such as type of plots, results tables, etc.</li> </ul> <p>In this lesson we will see a practical example on how you could organize your own files and folders.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#folder-organization","title":"Folder organization","text":"<p>You will probably want to divide your NGS data into three different types of folders:</p> <ol> <li>Assay folders: This folder contains the raw and processed NGS datasets, as well as the pipeline/workflow used to generate the processed data, provenance of the raw data and quality control reports of the data. This data should be locked and read-only to prevent unwanted modifications.</li> <li>Project folders: This folder contains all the necessary files for a specific research project. A project may use several assays or results from other projects. The assay data should not be copied or duplicated, but linked from the original source.</li> <li>Genomic resources folders: This folder contains common genomic resources, such as genome references (fasta files) and annotations (gtf files) for different species, as well as indexes for different alignment algorithms. This data should also be locked and read-only to prevent unwanted modifications.</li> </ol> <p>This will help you to keep your data tidied up, specially if you are working on a big lab where assays may be used for different purposes and different people!</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#assay-folder","title":"Assay folder","text":"<p>For each NGS experiment there should be an <code>Assay</code> folder that will contain all experimental datasets, that is, an <code>Assay</code> (raw files and pipeline processed files). Raw files should not be modified at all, but you should probably lock modifications to the final results once you are done with preprocessing the data. This will help you prevent unwanted modifications to the data. Each <code>Assay</code> subfolder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name an NGS assay using an acronym for the type of NGS assay (RNAseq, ChIPseq, ATACseq), a keyword that represents a unique descriptive element of that assay, and the date. Like this:</p> <pre><code>&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\n</code></pre> <p>For example <code>CHIP_Oct4_20230101</code> is a ChIPseq assay made on 1<sup>st</sup> January 2023 with the keyword Oct4, so it is easily identifiable by eye.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#assay-id-code-names","title":"Assay ID code names","text":"<p>Below is a example list of different Assay-ID code names. You are welcome to use it and expand it as you wish!</p> <ul> <li><code>CHIP</code>: ChIP-seq</li> <li><code>RNA</code>: RNA-seq</li> <li><code>ATAC</code>: ATAC-seq</li> <li><code>SCR</code>: scRNA-seq</li> <li><code>PROT</code>: Mass Spectrometry Assay</li> <li><code>CAT</code>: Cut&amp;Tag</li> <li><code>CAR</code>: Cut&amp;Run</li> <li><code>RIME</code>: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins</li> <li>...</li> </ul>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#assay-folder-structure","title":"Assay folder structure","text":"<p>Next, let's take a look at a possible folder structure and what kind of files you can find there.</p> <pre><code>CHIP_Oct4_20230101/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 metadata.yml\n\u251c\u2500\u2500 pipeline.md\n\u251c\u2500\u2500 processed\n\u2514\u2500\u2500 raw\n   \u251c\u2500\u2500 .fastq.gz\n   \u2514\u2500\u2500 samplesheet.csv\n</code></pre> <ul> <li>README.md: Long description of the assay in markdown format. It should contain provenance of the raw NGS data (samples, laboratory protocols used, aim of the assay, etc)</li> <li>metadata.yml: metadata file for the assay describing different keys and important information regarding that assay (see this lesson).</li> <li>pipeline.md: description of the pipeline used to process raw data, as well as the commands used to run the pipeline.</li> <li>processed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.</li> <li>raw: folder with the raw data.<ul> <li>.fastq.gz:In the case of NGS assays, there should be fastq files.</li> <li>samplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. You can also add extra columns with info regarding the experimental variables and batches so it can be used for downstream analysis as well.</li> </ul> </li> </ul> <p>This folder structure is simple and straight forward, and files will be exactly where you expect them to be! By including a description and metadata file, you will always be able to understand what this experiment was and how it was generated. This information will be specially useful once you need to submit your data for archiving or write a manuscript. The metadata file will be really handy if you want to collect information on all your NGS datasets and create a database (see this lesson)!</p> <p>Note that the processed folder is not expanded upon. This folder will be very dependant on the workflows/pipelines that you use. We strongly recommend that you use reproducible and community curated pipelines, such as the ones developed by the nf-core community, which have a through documentation on the results they produce. For example, imagine you have run an RNAseq experiment and processed the data using the nf-core:rnaseq pipeline with the pseudoalignment and quantification option:</p> <pre><code>processed/\n\u251c\u2500\u2500 fastqc/\n\u251c\u2500\u2500 multiqc/\n\u251c\u2500\u2500 pipeline_info/\n\u251c\u2500\u2500 salmon/\n\u2514\u2500\u2500 trimgalore/\n</code></pre> <ul> <li>fastqc: Quality Control results of the raw fastq files.</li> <li>multiqc: Full compilation of the Quality Control checks for all your samples for the entire pipeline.</li> <li>pipeline_info: Information and logs about the pipeline used for each of the steps of the workflow.</li> <li>salmon: Pseudoalignment and quantification results of the salmon algorithm.</li> <li>trimgalore: Cleaned fastq files and Quality Control results of the cleaned files.</li> </ul> <p>By using a standardized pipeline, you should be able to always find the preprocessing results where you expect them to be! No need to come up with your own file organization (and document it) or worry that your colleagues (or your future self) won't be able to find the data.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#project-folder","title":"Project folder","text":"<p>On the other hand, we have the other type of folder called <code>Projects</code>. In this folder you will save a subfolder for each project that you (or your lab) works on. Each <code>Project</code> subfolder will contain project information and all the data analysis notebooks and scripts used in that project.</p> <p>Projects and Assays are separated from each other because a project may use one or more assays to answer a scientific question, and assays may be reused several times in different projects. This could be, for example, all the data analysis related to a publication (a RNAseq and a ChIPseq experiment), or a comparison between a previous ATACseq experiment (which was used for a older project) with a new laboratory protocol.</p> <p>As like for an Assay folder, the Project folder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name it after the main author initials, a keyword that represents a unique descriptive element of that assay, and the date:</p> <pre><code>&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n</code></pre> <p>For example, <code>JARH_Oct4_20230101</code>, is a project about the gene Oct4 owned by Jose Alejandro Romero Herrera, created on the 1<sup>st</sup> of January of 2023.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#project-folder-structure","title":"Project folder structure","text":"<p>Next, let's take a look at a possible folder structure and what kind of files you can find there.</p> <pre><code>&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n\u251c\u2500\u2500 data\n\u2502  \u2514\u2500\u2500 &lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD/\n\u251c\u2500\u2500 documents\n\u2502  \u2514\u2500\u2500 Non-sensitive_NGS_research_project_template.docx\n\u251c\u2500\u2500 notebooks\n\u2502  \u2514\u2500\u2500 01_data_analysis.rmd\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 reports\n\u2502  \u251c\u2500\u2500 figures\n\u2502  \u2502  \u2514\u2500\u2500 01_data_analysis/\n\u2502  \u2502   \u2514\u2500\u2500 heatmap_sampleCor_20230102.png\n\u2502  \u2514\u2500\u2500 01_data_analysis.html\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 01_data_analysis/\n\u2502      \u2514\u2500\u2500 DEA_treat-control_LFC1_p01.tsv\n\u251c\u2500\u2500 scripts\n\u2514\u2500\u2500 metadata.yml\n</code></pre> <ul> <li>data: folder that contains symlinks or shortcuts to where the data is, avoiding copying and modification of original files.</li> <li>documents: folder containing word documents, slides or pdfs related to the project, such as explanations of the data or project, papers, etc. It also contains your Data Management Plan.<ul> <li>Non-sensitive_NGS_research_project_template.docx. This is a pre-filled Data Management Plan based on the Horizon Europe guidelines.</li> </ul> </li> <li>notebooks: folder containing Jupyter, R markdown or Quarto notebooks with the actual data analysis.</li> <li>README.md: detailed description of the project in markdown format.</li> <li>reports: notebooks rendered as html/docx/pdf versions, ideal for sharing with colleagues and also as a formal report of the data analysis procedure.<ul> <li>figures: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.</li> </ul> </li> <li>requirements.txt: file explaining what software and libraries/packages and their versions are necessary to reproduce the code.</li> <li>results: results from the data analysis, such as tables with differentially expressed genes, enrichment results, etc.</li> <li>scripts: folder containing helper scripts needed to run data analysis or reproduce the work of the folder</li> <li>description.yml: short description of the project.</li> <li>metadata.yml: metadata file for the assay describing different keys (see this lesson).</li> </ul> <p>Note on the data folder</p> <p>What do we mean with shortcuts to the data is? If you have divided your Assays and Projects as suggested, you do not really need to copy or duplicate your data again, simply make a shortcut to the original folder, and you will have access to the data as if it is in the project folder! You can use this command in Linux or MacOS to create a folder softlink:</p> <pre><code>ln -s path/to/assays/Assays/&lt;ASSAY_ID&gt; /path/to/projects/Projects/&lt;PROJECT_ID&gt;/data/\n</code></pre> <p>Note on the notebooks folder</p> <p>Using annotated notebooks is ideal for reproducibility and readability purposes. Notebooks should be labeled numerically in order they were created, or the order of the data analysis steps, if they are related to each other e.g. <code>00_preprocessing.rmd</code> - <code>01_Differential_Expression_Analysis.rmd</code> - <code>02_Functional_Analysis.rmd</code></p> <p>Note on the results folder</p> <p>Results from your code notebooks should be saved under this folder. Create a subfolder named after the notebook that created them, so you can always identify which notebook created which results!</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#template-engine","title":"Template engine","text":"<p>You may be thinking right now that doing this kind of setup every time you need to create a new folder would be very tedious, and you would be right! Fortunately, it is very easy to create a folder template using cookiecutter. Cookiecutter is a command-line utility that creates projects from cookiecutters (that is, a template), e.g. creating a Python package project from a Python package project template. Here you can find an example of a cookiecutter folder template directed to NGS data, where we have applied the structures explained in the previous sections. You are very welcome to adapt it or modify it to your needs!</p> <p>Now, using only cookiecutter might have some issues. If you update the template, it might be really hard to maintain older folders. By using another tool, cruft, when generating assay and project folders, it allows the user to validate and syncronize old templates with the latest version.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#quick-tutorial-on-cookiecutter","title":"Quick tutorial on cookiecutter","text":"<p>Creating a Cookiecutter template from scratch involves defining a folder structure, creating a <code>cookiecutter.json</code> file, and specifying the placeholders (keywords) that will be replaced during project generation. Let's walk through the process step by step:</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#step-1-create-a-folder-template","title":"Step 1: Create a Folder Template","text":"<p>Start by creating a folder with the structure you want for your template. For example, let's create a simple Python project template:</p> <pre><code>my_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\n</code></pre> <p>In this example, <code>{{cookiecutter.project_name}}</code> is a placeholder that will be replaced with the actual project name when the template is used.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#step-2-create-cookiecutterjson","title":"Step 2: Create <code>cookiecutter.json</code>","text":"<p>In the root of your template folder, create a file named <code>cookiecutter.json</code>. This file will define the variables (keywords) that users will be prompted to fill in. For our Python project template, it might look like this:</p> <pre><code>{\n  \"project_name\": \"MyProject\",\n  \"author_name\": \"Your Name\",\n  \"description\": \"A short description of your project\"\n}\n</code></pre> <p>These are the questions users will be asked when generating a project based on your template. The values provided here will be used to replace the corresponding placeholders in the template files.</p> <p>In addition to replacing placeholders in file and directory names, Cookiecutter can also automatically fill in information within the contents of text files. This can be useful for providing default configurations or templates for code files. Let's extend our previous example to include a placeholder inside a text file:</p> <p>First, modify the <code>my_template/main.py</code> file to include a placeholder inside its contents:</p> <pre><code># main.py\n\ndef hello():\n    print(\"Hello, {{cookiecutter.project_name}}!\")\n</code></pre> <p>Now, the <code>{{cookiecutter.project_name}}</code> placeholder is inside the <code>main.py</code> file. When you run Cookiecutter, it will automatically replace the placeholders not only in file and directory names but also within the contents of text files. After running Cookiecutter, your generated <code>main.py</code> file might look like this:</p> <pre><code># main.py\n\ndef hello():\n    print(\"Hello, MyProject!\")  # Assuming \"MyProject\" was entered as the project_name\n</code></pre>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#step-3-use-cookiecutter","title":"Step 3: Use Cookiecutter","text":"<p>Now that your template is set up, you can use Cookiecutter to generate a project based on it. Open a terminal and run:</p> <pre><code>cookiecutter path/to/your/template\n</code></pre> <p>Cookiecutter will prompt you to fill in the values for <code>project_name</code>, <code>author_name</code>, and <code>description</code>. After you provide these values, Cookiecutter will replace the placeholders in your template files with the entered values.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#step-4-explore-the-generated-project","title":"Step 4: Explore the Generated Project","text":"<p>Once the generation process is complete, navigate to the directory where Cookiecutter created the new project. You will see a project structure with the placeholders replaced by the values you provided.</p> <p>Exercise: Create your own template</p> <p>Using cookiecutter, create your own templates for your folders. You do not need to copy exactly our suggestions, adjust your template to your own needs! In order to create your cookiecutter template, your will need to install python, cookiecutter, Git and a GitHub account. If you do not have Git and a GitHub account, we suggest you do one as soon as possible. We will take a deeper look at Git and GitHub in the version control lesson.</p> <p>We have prepared already two simple cookiecutter templates in GitHub repositories.</p> <p>Assay</p> <ol> <li>First, fork our <code>https://github.com/hds-sandbox/assay-template</code> from the GitHub page into your own account/organization. </li> <li>Then, use <code>git clone &lt;your URL to the template&gt;</code> to put it in your computer.</li> <li>Modify the contents of the repository so that it matches the Assay example above. You are welcome to do changes as you please!</li> <li>Modify the <code>cookiecutter.json</code> file so that it will include the Assay name template</li> <li>Git add, commit and push your changes</li> <li>Test your folder by using <code>cookiecutter &lt;URL to your GitHub repository for \"assay-template&gt;</code></li> </ol> <p>Project</p> <ol> <li>First, fork our <code>https://github.com/hds-sandbox/project-template</code> from the GitHub page into your own account/organization. </li> <li>Then, use <code>git clone &lt;your URL to the template&gt;</code> to put it in your computer.</li> <li>Modify the contents of the repository so that it matches the Project example above. You are welcome to do changes as you please!</li> <li>Modify the <code>cookiecutter.json</code> file so that it will include the Project name template</li> <li>Git add, commit and push your changes</li> <li>Test your folder by using <code>cookiecutter &lt;URL to your GitHub repository for \"project-template&gt;</code></li> </ol>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#genomic-resources-folder","title":"Genomic resources folder","text":"<p>Preprocessing NGS data usually requires different genomic resources in order to align and and annotate fastq files. First and most importantly, you will need reference genomes (human and mouse are the most common ones) in FASTA format. In addition, aligner tools such as STAR, Bowtie, etc., require indexed fasta files to perform alignment of reads. Moreover, GTF or GFF files are necessary in order to quantify reads into genomic regions (such as genes or promoters). Nonetheless, these genomic resources are often updated and are released periodically under different versions from different sources. In order to make your data reproducible, you will need to control and manage these genomic resources and specify their versions and sources. For example, the latest mouse reference genome is GRCm39, but may studies still align their reads to the version GRCm38.</p> <p>How do you keep track of your resources? You could, again, set up a folder structure that hosts reference genomes, annotations and indexes per species and per version, or use a reference genome manager such as refgenie. We will take a look at both options.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#refgenie","title":"RefGenie","text":"<p>Refgenie manages storage, access, and transfer of reference genome resources. It provides command-line and Python interfaces to download pre-built reference genome \"assets\", like indexes used by bioinformatics tools. It can also build assets for custom genome assemblies. Refgenie provides programmatic access to a standard genome folder structure, so software can swap from one genome to another.</p> <p>Check their tutorial if you want to start managing your genomic resources with Refgenie!</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#manual-download","title":"Manual download","text":"<p>If you want to keep manual track and manage yourself and not depend on a third party tool, you can always download the genomic resources directly from the source. We will show you how in this section:</p> <ol> <li>Create a new folder, called <code>genomic_resources</code>.</li> <li>Inside this folder, separate each species with a subfolder.</li> <li>Inside the species subfolder, separate it by the resource version</li> <li>Inside the version subfolder, separate genomic sequences (FASTA) and annotations (GTF/GFF) from different alignment indexes</li> </ol> <p>In the example below, we are downloading human (version GRCh38) and mouse (version GRCm39) genomic resources from the ensembl FTP server</p> <pre><code>genomic_resources/\n\u251c\u2500\u2500 homo_sapiens/\n\u2502  \u2514\u2500\u2500 GRCh38/\n\u2502     \u251c\u2500\u2500 Homo_sapiens.GRCh38.109.gtf.gz\n\u2502     \u251c\u2500\u2500 Homo_sapiens.GRCh38.109.dna_sm.primary_assembly.fa.gz\n\u2502     \u2514\u2500\u2500 indexes/\n\u2502        \u251c\u2500\u2500 salmon/\n\u2502        \u2514\u2500\u2500 STAR/\n\u251c\u2500\u2500 mus_musculus/\n\u2502  \u2514\u2500\u2500 GRCm39/\n\u2502     \u251c\u2500\u2500 Mus_musculus.GRCm39.109.gtf.gz\n\u2502     \u251c\u2500\u2500 Mus_musculus.GRCm39.109.dna_sm.primary_assembly.fa.gz\n\u2502     \u2514\u2500\u2500 indexes/\n\u2502        \u251c\u2500\u2500 salmon/\n\u2502        \u2514\u2500\u2500 STAR/\n\u251c\u2500\u2500 ref_genomes.sh\n\u2514\u2500\u2500 create_indexes.sh\n</code></pre> <p>As you can see, we have also keep here a bash script that downloads and sets up the folders. The contents of the bash script should also indicate a date of when was the data downloaded. For example:</p> <pre><code>#!/bin/bash\n\n# Download human (GRCh38) and mouse (GRCm39) genome and annotations\n# Release 109\n# 2023-07-10\n\n# Go to home\ncd\n# Create homo_sapiens version folder and go to that folder\nmkdir -p genomic_resources/homo_sapiens/GRCh38\ncd  genomic_resources/homo_sapiens/GRCh38\n\n# Download FASTA file and GTF file from the ensembl release 109\nwget -L ftp://ftp.ensembl.org/pub/release-109/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-109/gtf/homo_sapiens/Homo_sapiens.GRCh38.109.gtf.gz\n\n# Go to home\ncd\n# Create mus_musculus version folder and go to that folder\nmkdir -p genomic_resources/mouse/GRCm39\ncd  genomic_resources/mouse/GRCm39\n\n# Download FASTA file and GTF file from the ensembl release 109\nwget -L ftp://ftp.ensembl.org/pub/release-109/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna_sm.primary_assembly.fa.gz\nwget -L https://ftp.ensembl.org/pub/release-109/gtf/mus_musculus/Mus_musculus.GRCm39.109.gtf.gz\n</code></pre> <p>After that is all set up, you can run another bash script that will create a index folder inside each version subfolder and generate indexes for different alignment tools!</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#naming-conventions","title":"Naming conventions","text":"<p>Using consistent naming conventions is important in scientific research as it helps with the organization and retrieval of data or results. By adopting standardized naming conventions, researchers ensure that files, experiments, or data sets are labeled in a clear, logical manner. This makes it easier to locate and compare similar types of data or results, even when dealing with large datasets or multiple experiments. For instance, in genomics, employing uniform naming conventions for files related to specific experiments or samples allows for swift identification and comparison of relevant data, streamlining the research process and contributing to the reproducibility of findings. This practice promotes efficiency, collaboration, and the integrity of scientific work.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#general-tips","title":"General tips","text":"<p>Below you will find a small list of general tips to follow when you name a folder or a file:</p> <ul> <li>Use only alphanumeric characters to write a word: a to z and 0 to 9</li> <li>Avoid special characters: ~!@#$%^&amp;*()`[]{}\"|</li> <li>Date format: use <code>YYYYMMDD</code> format. For example: 20230101.</li> <li>Authors: use initials. For example: JARH</li> <li>Don't use of spaces! Computers get very confused when you need to point a path to a file and it contains spaces! Instead:<ul> <li>Separate field sections are separated by underscores <code>_</code>.</li> <li>Words in each section are written in camelCase.   It would look then like this: <code>field1_word1Word2.txt</code>. For example: <code>heatmap_sampleCor_20230101.png</code>. The first field indicates what this file is, i.e., a heatmap. Second field is what is being plotted, i.e, sample correlations; since the field contains two words, they are written in camelCase. The third field is the date of when the image was created.</li> </ul> </li> <li>Use as short fields as possible. You can try to use understandable abbreviations, like LFC for LogFoldChange, Cor for correlations, Dist for distances, etc.</li> <li>Avoid long names as much as you can, be concise!</li> <li>Avoid creating many sublevels of folders.</li> <li>Write down your naming convention pattern and document it in the README file</li> <li>When using a sequential numbering system, use leading zeros to make sure files sort in sequential order. Using <code>01</code> instead of just <code>1</code> if your sequence only goes up to <code>99</code>.</li> <li>Versions should be used as the last element, and use at least two digits with a leading 0 (e.g. v01, v02)</li> </ul>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#suggestions-for-ngs-data","title":"Suggestions for NGS data","text":"<p>More info on naming conventions for different types of files and analysis is in development.</p> name description naming_convention file format example .fastq raw sequencing reads nan nan sampleID_run_read1.fastq .fastqc quality control from fastqc nan nan sampleID_run_read1.fastqc .bam aligned reads nan nan sampleID_run_read1.bam GTF sequence annotation nan nan one of https://www.gencodegenes.org/ GFF sequence annotation nan nan one of https://www.gencodegenes.org/ .bed genome locations nan nan nan .bigwig genome coverage nan nan nan .fasta sequence data (nucleotide/aminoacid) nan nan one of https://www.gencodegenes.org/ Multiqc report QC aggregated report &lt;assayID&gt;_YYYYMMDD.multiqc multiqc RNA_20200101.multiqc Count matrix final count matrix &lt;assayID&gt;_cm_aligner_YYYYMMDD.tsv tsv RNA_cm_salmon_20200101.tsv DEA differential expression analysis results DEA_&lt;condition1-condition2&gt;_LFC&lt;absolute_threshold&gt;_p&lt;pvalue decimals&gt;_YYYYMMDD.tsv tsv DEA_treat-untreat_LFC1_p01_20200101.tsv DBA differential binding analysis results DBA_&lt;condition1-condition2&gt;_LFC&lt;absolute_threshold&gt;_p&lt;pvalue decimals&gt;_YYYYMMDD.tsv tsv DBA_treat-untreat_LFC1_p01_20200101.tsv MAplot MA plot MAplot_&lt;condition1-condition2&gt;_YYYYMMDD.jpeg jpeg MAplot_treat-untreat_20200101.jpeg Heatmap plot Heatmap plot of anything heatmap_&lt;type&gt;_YYYYMMDD.jpeg jpeg heatmap_sampleCor_20200101.jpeg Volcano plot Volcano plot volcano_&lt;condition1-condition2&gt;_YYYYMMDD.jpeg jpeg volcano_treat-untreat_20200101.jpeg Venn diagram Venn diagram venn_&lt;type&gt;_YYYYMMDD.jpeg jpeg venn_consensus_20200101.jpeg Enrichment table Enrichment results nan tsv nan <p>Exercise: Create your own naming conventions</p> <p>Think about the most common types of files and folders you will be working on, such as visualizations, results tables, processed files, etc. Then come up with a logical and clear way of naming those files using the tips suggested above. Remember to avoid making long and complicated names!</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"06_file_structure.html#wrap-up","title":"Wrap up","text":"<p>In this lesson, we have learned some practical tips and examples about how to organize your data and bring some order to chaos! We have shown how to use <code>cookiecutter</code> as a template engine that can be used to create new <code>Assay</code> and <code>Project</code> folders without hassle. Hopefully you are now able to create your own templates and reuse them as much as you need. In the next lesson, we will look at what kind of metadata you should record for your <code>Assay</code> and <code>Project</code> folders, from general fields such as author or date, to more NGS specific like organism, cell type or tissue.</p>","tags":["File structure","File organization","Naming conventions","Folder templates"]},{"location":"07_metadata.html","title":"Metadata for NGS data","text":""},{"location":"07_metadata.html#metadata-for-ngs-data","title":"Metadata for NGS data","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Learn what is metadata  </li> <li>Learn what metadata is associated to NGS data</li> <li>Learn sources for controlled vocabularies for NGS data</li> </ol> <p>Metadata is the behind-the-scenes information that makes sense of data. It's the extra layer of details that help you understand what the data is all about. Imagine you have a book \u2013 the words on the pages are the data, while the title, author, and table of contents are the metadata. In a similar way, metadata gives data context and structure. For NGS data, metadata includes information such as when and where the data was collected, what it represents, and how it was processed. In this section, we will explore what kind of relevant metadata is available for NGS data, and how to capture it in your Assays or Project folders.</p>"},{"location":"07_metadata.html#what-is-metadata-and-why-it-is-important","title":"What is metadata and why it is important","text":"<p> From ontotext.com</p> <p>Imagine you have a batch of DNA sequences from different people. The raw sequences are like jigsaw puzzle pieces, and metadata is a cheat sheet that tells you which pieces fit where. It could include details like when and where the samples were collected, the lab procedures used, who did created them, and even the equipment involved, providing context and making sense of the who, what, when, where, and why.</p> <p>Definition of metadata</p> <p>Metadata refers to data that provides information about other data. It describes various aspects of the data, such as its origin, structure, format, and context. Metadata is typically used to facilitate the organization, management, and interpretation of data, making it easier to locate, access, and understand. In essence, metadata adds valuable context and attributes to the primary data, enhancing its usability and ensuring efficient data management.\"</p> <p>Let's think of an example that shows why metadata is extremelly important. Imagine you're in a big lab with a plethora of different datasets all saved under generic folder names. Without metadata, it would be like searching for a needle in a haystack. You'd have folders labeled 'Experiment123,' 'DataBatch42,' and so on, but zero clues about what's inside. With metadata, 'Experiment123' is not just that anymore, but 'DNA Sequencing of Human Cells, March 2023'. Providing relevant metadata converts data chaos into an organized repository, turning data exploration, interpretation, and insight extraction into a much easier journey. This is true not only for yourself or your lab, but to other researchers that might want to reuse your data. Collecting metadata from the very beginning of the research project will help tremendously to alleviate future efforts to understand the data. It can also help you make an organised collection of data, so that you do not lose any information regarding the data, or that you do not repeat an unnecessary experiment that someone else has done it before, but you could not find. That is, it helps you save money and time!</p> <p>Benefits of collecting proper metadata</p> <ol> <li>Data Context and Interpretation: Metadata provides crucial context to NGS data, offering insights into the experimental conditions, sample origins, and processing methods. This context is vital for understanding data variations, drawing accurate conclusions, and interpreting results correctly.</li> <li>Data Discovery and Access: With metadata, researchers can easily locate and access specific NGS datasets within large repositories. Details like sample identifiers, experimental parameters, and timestamps help researchers quickly identify relevant data for their analyses.</li> <li>Reproducibility and Collaboration: Metadata ensures that NGS experiments can be replicated and validated by others. By sharing comprehensive metadata, researchers enable colleagues to reproduce analyses, compare results, and collaborate effectively, bolstering the integrity of scientific findings.</li> <li>Quality Control and Validation: Metadata supports data quality assessment by allowing researchers to track the origin and handling of NGS data. It enables the identification of potential errors or biases, helping researchers validate the accuracy and reliability of their analyses.</li> <li>Long-Term Data Preservation: Properly documented metadata is essential for preserving NGS data over time. As research evolves, metadata ensures that future generations can understand and utilize archived NGS datasets, ensuring the continued impact of scientific discoveries.</li> </ol>"},{"location":"07_metadata.html#how-to-collect-metadata-in-your-folders","title":"How to collect metadata in your folders","text":"<p>In our previous lesson, we learnt about how to organize your data into different types of folders: <code>Assays</code> and <code>Projects</code>. Both of these folders contain a metadata.yml file and a README.md file. In this section, we will check what kind of information you should collect in each of these files.</p>"},{"location":"07_metadata.html#readmemd-file","title":"README.md file","text":"<p>The README.md file is a markdown file that allows you to write a long description of the data placed in a folder. Since it is a markdown file, you are able to write in rich text format (bold, italic, include links, etc) what is inside the folder, why it was created/collected, how and when. If it is an <code>Assay</code> folder, you could include the laboratory protocol used to generate the samples, images explaining the experiment design, a summary of the results of the experiment and any sort of comments that would help to understand the context of the experiment. On the other hand, a 'Project' README file may contain a description of the project, what are its aims, why is it important, what 'Assays' is it using, how to interpret the code notebooks, a summary of the results and, again, any sort of comments that would help to understand the project.</p> <p>Here is an example of a README file for a `Project`` folder:</p> <pre><code># NGS Analysis Project: Exploring Gene Expression in Human Tissues\n\n## Aims\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\n\n## Why It's Important\n\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n## Datasets\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\n\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n## Summary of Results\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\n\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n---\n\nFor more details, refer to our [Jupyter Notebook](link-to-jupyter-notebook.ipynb) for the complete analysis pipeline and code.\n</code></pre>"},{"location":"07_metadata.html#metadatayml","title":"metadata.yml","text":"<p>The metadata file is a yml file, which is a text document that contains data formatted using a human-readable data format for data serialization. It is not mandatory that you use yml format, any other structured file format will work too, such as json files. We recommend using yml format because it is easily readable for humans, so non-coding people will have an easier time checking, writing or modifying the file if they need to.</p> <p></p>"},{"location":"07_metadata.html#metadata-fields","title":"Metadata fields","text":"<p>There is a ton of information you can collect regarding an NGS assay or a project. Some information fields are very general, such as author or date, while others are specific to the Assay or Project folder. Below, we will take a look at minimal information you should collect in each of the folders. But before that, we need to talk about controlled vocabularies/ontologies.</p> <p>Imagine this scenario: a researcher in genomics us excited to explore various NGS datasets from different human tissue samples to study gene expression patterns. However, she encounters a significant hurdle: the tissue names used in the datasets are inconsistent and lack standardized terms. Some datasets refer to \"brain,\" while others use \"cerebral cortex\" or \"cortical tissue.\" This lack of controlled vocabularies for tissue names complicates her data integration efforts, requiring her to spend additional time curating and mapping tissue labels to establish meaningful cross-dataset comparisons.</p> <p>So, what happened here?  If the original creators of the NGS datasets had adopted a standardized vocabulary for tissue names, Dr. Smith could have seamlessly integrated the data without the need for extensive curation. By employing a widely accepted tissue ontology, like the Uberon Ontology, dataset contributors could have consistently used predefined terms, such as \"brain\" or \"cerebral cortex.\" This practice would have not only simplified data integration but also facilitated accurate cross-dataset comparisons and enabled more reliable downstream analyses.</p> <p>In the context of NGS data, ontologies and controlled vocabularies play a pivotal role in clarifying and categorizing many concepts and information about your data. More examples are: the Gene Ontology (GO), which provides a shared vocabulary to describe gene functions, molecular processes, and cellular components, enhancing the consistency and comparability of NGS results, and Ensembl gene IDs, which are used to uniquely represent individual genes and provide a standardized way of referencing and accessing gene-related information across various species. By leveraging ontologies, researchers ensure that metadata fields capture specific details consistently across experiments, from sample sources and protocols to experimental conditions.</p> <p>Definition of ontology</p> <p>An ontology is a formal representation of knowledge that encompasses concepts, their attributes, and the relationships between them within a particular domain or subject area. It serves as a structured framework for organizing and categorizing information, facilitating the interpretation, sharing, and integration of knowledge across diverse applications and disciplines. Ontologies employ standardized vocabularies and define the semantics of terms, enabling effective communication and reasoning among humans and computer systems. They play a pivotal role in knowledge representation, data integration, and semantic interoperability, contributing to enhanced understanding, collaboration, and analysis within complex domains.</p> <p>This standardization not only enhances data discoverability and interoperability but also empowers robust data analysis, accelerates knowledge sharing, and enables meaningful cross-study comparisons. In essence, ontologies serve as the universal translators of the scientific language, fostering a harmonious symphony of data interpretation and collaboration.</p>"},{"location":"07_metadata.html#general-metadata-fields","title":"General metadata fields","text":"<p>Here you can find a list of suggestions for general metadata fields that can be used for both assays and project folders:</p> <ul> <li>Title: A brief yet informative name for the dataset.</li> <li>Author(s): The individual(s) or organization responsible for creating the dataset. You can use your ORCID</li> <li>Date Created: The date when the dataset was originally generated or compiled. Use YYYY-MM-DD format!</li> <li>Date Modified: The date when the dataset was last updated or modified. Use YYYY-MM-DD format!</li> <li>Object ID: The project or assay ID for tracking and reference purposes.</li> <li>Description: A short narrative explaining the content, purpose, and context.</li> <li>Keywords: A set of descriptive terms or phrases that capture the folder's main topics and attributes.</li> <li>Ethical and Legal Considerations: Information about ethical approvals, consent, and any legal restrictions.</li> <li>Version: The version number or identifier for the folder, useful for tracking changes.</li> <li>Related Publications: Links or references to any scientific publications associated with the folder. Try to add here the DOI!</li> <li>Funding Source: Details about the funding agency or source that supported the research and data generation.</li> <li>License: The type of license or terms of use associated with the dataset/project.</li> <li>Contact Information: Contact details for individuals who can provide further information about the dataset/project.</li> </ul>"},{"location":"07_metadata.html#assay-metadata-fields","title":"Assay metadata fields","text":"<p>Here you will find a table with possible metadata fields that you can use to annotate and track your <code>Assay</code> folders:</p> Metadata field Definition Format Ontology Example assay_ID Identifier for the assay that is at least unique within the project &lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD nan CHIP_Oct4_20200101 assay_type The type of experiment performed, eg ATAC-seq or seqFISH nan ontology field- e.g. EFO or OBI ChIPseq assay_subtype More specific type or assay like bulk nascent RNAseq or single cell ATACseq nan ontology field- e.g. EFO or OBI bulk ChIPseq owner Owner of the assay (who made the experiment?). &lt;First Name&gt; &lt;Last Name&gt; nan Jose Romero platform The type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform nan ontology field- e.g. EFO or OBI Illumina extraction_method Technique used to extract the nucleic acid from the cell nan ontology field- e.g. EFO or OBI nan library_method Technique used to amplify a cDNA library nan ontology field- e.g. EFO or OBI nan external_accessions Accession numbers from external resources to which assay or protocol information was submitted nan eg protocols.io, AE, GEO accession number, etc GSEXXXXX keyword Keyword for easy identification wordWord camelCase Oct4ChIP date Date of assay creation YYYYMMDD nan 20200101 nsamples Number of samples analyzed in this assay &lt;integer&gt; nan 9 is_paired Paired fastq files or not &lt;single OR paired&gt; nan single pipeline Pipeline used to process data and version nan nan nf-core/chipseq -r 1.0 strandedness The strandedness of the cDNA library &lt;+ OR - OR *&gt; nan * processed_by Who processed the data &lt;First Name&gt; &lt;Last Name&gt; nan Sarah Lundregan organism Organism origin &lt;Genus species&gt; Taxonomy name Mus musculus origin Is internal or external (from a public resources) data &lt;internal OR external&gt; nan internal path Path to files &lt;/path/to/file&gt; nan nan short_desc Short description of the assay plain text nan Oct4 ChIP after pERK activation ELN_ID ID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling plain text nan nan"},{"location":"07_metadata.html#sample-metadata-fields","title":"Sample metadata fields","text":"<p>There is some information that will be specific to your samples. For example, which samples are treated, which are control, which tissue do they come from, which cell type, etc. In this case, it would be beneficial if you include all this information in the <code>samplesheet.csv</code> that describes the fastq files. Here is a list of possible metadata fields that you can use:</p> Metadata field Definition Format Ontology Example sample Name of the sample nan nan control_rep1, treat_rep1 fastq_1 Path to fastq file 1 nan nan AEG588A1_S1_L002_R1_001.fastq.gz fastq_2 Path to paired fastq file, if it is a paired experiment nan nan AEG588A1_S1_L002_R2_001.fastq.gz strandedness The strandedness of the cDNA library &lt;unstranded OR forward OR reverse &gt; nan unstranded condition Variable of interest of the experiment, such as \"control\", \"treatment\", etc wordWord camelCase control, treat1, treat2 cell_type The cell type(s) known or selected to be present in the sample nan ontology field- e.g. EFO or OBI nan tissue The tissue from which the sample was taken nan Uberon nan sex The biological/genetic sex of the sample nan ontology field- e.g. EFO or OBI nan cell_line Cell line of the sample nan ontology field- e.g. EFO or OBI nan organism Organism origin of the sample Taxonomy Mus musculus replicate Replicate number &lt;integer&gt; nan 1 batch Batch information wordWord camelCase 1 disease Any diseases that may affect the sample nan Disease Ontology or MONDO nan developmental_stage The developmental stage of the sample nan nan nan sample_type The type of the collected specimen, eg tissue biopsy, blood draw or throat swab nan nan nan strain Strain of the species from which the sample was collected, if applicable nan ontology field - e.g. NCBITaxonomy nan genetic variation Any relevant genetic differences from the specimen or sample to the expected genomic information for this species, eg abnormal chromosome counts, major translocations or indels nan nan nan"},{"location":"07_metadata.html#project-metadata-fields","title":"Project metadata fields","text":"<p>Here you will find a table with possible metadata fields that you can use to annotate and track your <code>Project</code> folders:</p> <p>In development.</p> Metadata field Definition Format Ontology Example project Project ID &lt;surname&gt;_et_al_2023 nan proks_et_al_2023 author Owner of the project &lt;First name&gt; &lt;Surname&gt; nan Martin Proks date Date of creation YYYYMMDD nan 20230101 description Short description of the project Plain text nan This is a project describing the effect of Oct4 perturbation after pERK activation"},{"location":"07_metadata.html#more-info","title":"More info","text":"<p>The information provided in this lesson is not at all exhaustive. There might be many more fields and controlled vocabularies that could be useful for your NGS data. We recommend that you take a look at the following sources for more information!</p> <ul> <li>Transcriptomics metadata standards and fields</li> <li>Bionty: Biological ontologies for data scientists.</li> </ul> <p>Exercise: modify the metadata files in your cookiecutter templates</p> <p>We have seen some examples of metadata for NGS data. It is time now to customize your cookiecutter templates and modify the metadata.yml files so that they fit your needs! </p> <ol> <li>Think about what kind of metadata you would like to include.</li> <li>Modify the <code>cookiecutter.json</code> file so that when you create a new folder template, all the metadata is filled accordingly. </li> <li>Modify the <code>metadata.yml</code> file so that it includes the metadata recorded by the <code>cookiecutter.json</code> file. </li> <li>Modify the <code>README.md</code> file so that it includes the short description recorded by the <code>cookiecutter.json</code> file.</li> <li>Git add, commit and push the changes of your template.</li> <li>Test your folders by using the command <code>cookiecutter &lt;URL to your cookiecutter repository in GitHub&gt;</code></li> </ol>"},{"location":"07_metadata.html#wrap-up","title":"Wrap up","text":"<p>In this lesson we have learned about what metadata should be attached to your data in order to be reusable and understood in the future. Not only that, the metadata provided can help to process your samples adequately, and could even be useful for metadata studies. We have also briefly introduced some controlled vocabularies and sources for different fields, such as Disease Ontologies, Cell type ontologies, Organisms, etc. However, since this course is not aimed at enforcing the use of these vocabularies (that would be very complex), their implementation and use is up to you! In the next lesson we will learn how we can collect all the metadata information in each of the folders in order to make a database of assays and projects, allowing you to browse all your data so that they are always at hand!</p>"},{"location":"08_database.html","title":"Creating a data catalog","text":""},{"location":"08_database.html#database-of-your-ngs-experiments-and-projects","title":"Database of your NGS experiments and projects","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Learn how to create a database based on your metadata files</li> <li>Create and use a catalogue browser to explore your data files</li> </ol> <p>Imagine the same lab as before, that lab with a ton of NGS data with lots of assays and projects folders, as we have seen in the previous section. Now, picture a researcher eager to test a hypothesis with a new NGS experiment. But here's the catch: they're unsure if a similar experiment has already been conducted in the past. This uncertainty leads to hours lost looking through the data, only to realize they're reinventing the wheel. Or even worse! They performed the experiment and now a colleague remembers that the experiment was done some time ago by someone that left the lab already. Not only time was wasted, but also money! Now, let's rewind and introduce a centralized database cataloging every NGS experiment and projects using those NGS data. In this section, we explore how we can create a database from the 'metadata.yml' files we have created for both project and assays folder and see how this simple tool will let you and your lab control your all the data saved in your storage.</p>"},{"location":"08_database.html#what-is-a-database","title":"What is a database","text":"<p>A database is a structured repository that stores, manages, and retrieves information. Databases are the basis of efficient data organization, ensuring data integrity and accessibility. You could store your database in something as simple as a table in tabular format (like excel, tsv, csv), although it would be more appropriate to use a DataBase Management System (DBMS) like SQLite. A DBMS would allow you to store data efficiently, more securely and be able to make fast queries of data. That said, it is very possible that you do not need all these features, and a browsable table would be enough! It all depends on the complexity and number of experiments and folders you have. We will take a look at both possibilities, always trying to make it as simple as possible.</p>"},{"location":"08_database.html#using-a-table-as-a-database","title":"Using a table as a database","text":"<p>Since all our metadata files are structured in the same way, it would be very simple to construct a table by recursively going through your main <code>Assays</code> or <code>Projects</code> folder with an R or python script, and saving it as <code>database_YYYYMMDD.tsv</code>. We would recommend to use tabs instead of commas as your separator, due to the fact that some fields (like short description, or lists of keywords, for example) may contain commas.</p> <p>For example, consider this <code>Assays</code> folder:</p> <pre><code>Assays/\n\u251c\u2500\u2500 CHIP_oct4_20200101/\n\u251c\u2500\u2500 RNA_oct4_20200101/\n\u251c\u2500\u2500 CHIP_med1_20190204/\n\u251c\u2500\u2500 SCR_humanSkin_20210302/\n\u2514\u2500\u2500 SCR_humanBrain_20220610/\n</code></pre> <p>You can use an R script like this one to create a tsv table:</p> <pre><code>library(yaml)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n  file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n  metadata_list &lt;- lapply(file_list, yaml::yaml.load_file)\n  return(metadata_list)\n}\n\n# Specify the folder path\nfolder_path &lt;- \"/path/to/your/folder\"\n\n# Fetch metadata from the specified folder\nmetadata &lt;- get_metadata(folder_path)\n\n# Convert metadata to a data frame\nmetadata_df &lt;- data.frame(matrix(unlist(metadata), ncol = length(metadata), byrow = TRUE))\ncolnames(metadata_df) &lt;- names(metadata[[1]])\n\n# Save the data frame as a TSV file\noutput_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".tsv\")\nwrite.table(metadata_df, file = output_file, sep = \"\\t\", quote = FALSE, row.names = FALSE)\n\n# Print confirmation message\ncat(\"Database saved as\", output_file, \"\\n\")\n</code></pre> <p>Your database table should look like this:</p> assay_ID assay_type assay_subtype owner platform extraction_method library_method external_accessions keyword date nsamples is_paired pipeline strandedness processed_by organism origin path short_desc ELN_ID RNA_oct4_20200101 RNAseq bulk RNAseq Sarah Lundregan NextSeq 2000 nan nan nan oct4 20200101 9 paired nf-core/chipseq 2.3.1 * SL Mus musculus internal nan Bulk RNAseq of Oct4 knockout 234 CHIP_oct4_20200101 ChIPseq bulk ChIPseq Jose Romero NextSeq 2000 nan nan nan oct4 20200101 9 single nf-core/rnaseq 3.12.0 * JARH Mus musculus internal nan Bulk ChIPseq of Oct4 overexpression 123 CHIP_med1_20190204 ChIPseq bulk ChIPseq Martin Proks NextSeq 2000 nan nan nan med1 20190204 12 single nf-core/rnaseq 3.12.0 * MP Mus musculus internal nan Bulk ChIPseq of Med1 overexpression 345 SCR_humanSkin_20210302 RNAseq single cell RNAseq Jose Romero NextSeq 2000 nan nan nan humanSkin 20210302 23123 paired nf-core/scrnaseq 1.8.2 * JARH Homo sapiens external nan scRNAseq analysis of human skin development nan SCR_humanBrain_20220610 RNAseq single cell RNAseq Martin Proks NextSeq 2000 nan nan nan humanBrain 20220610 1234 paired custom * MP Homo sapiens external nan scRNAseq analysis of human brain development nan <p>Using a tabular table is a very simple way to create a catalog of all your projects and assays, making sure your lab can keep track of all the experiments ever performed.</p>"},{"location":"08_database.html#using-a-sqlite-database","title":"Using a SQLite database","text":"<p>SQLite is a lightweight and self-contained relational database management system known for its simplicity and efficiency. Unlike traditional databases, SQLite doesn't require a separate server to operate, making it a convenient choice for smaller-scale applications and scenarios where resource usage needs to be minimal, like our case! It excels in tasks that involve storing and retrieving structured data, offering a reliable solution for managing information such as the metadata collected in our experiments.</p> <p>All the information saved in the metadata.yml files can be recorded in a SQLite database with a script like this:</p> <pre><code>library(yaml)\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n  file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n  metadata_list &lt;- lapply(file_list, yaml::yaml.load_file)\n  return(metadata_list)\n}\n\n# Specify the folder path\nfolder_path &lt;- \"/path/to/your/folder\"\n\n# Fetch metadata from the specified folder\nmetadata &lt;- get_metadata(folder_path)\n\n# Convert metadata to a data frame\nmetadata_df &lt;- data.frame(matrix(unlist(metadata), ncol = length(metadata), byrow = TRUE))\ncolnames(metadata_df) &lt;- names(metadata[[1]])\n\n# Create an SQLite database and insert data\ndb_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".sqlite\")\ncon &lt;- dbConnect(SQLite(), db_file)\n\ndbWriteTable(con, \"metadata\", metadata_df, row.names = FALSE)\n\n# Print confirmation message\ncat(\"Database saved as\", db_file, \"\\n\")\n\n# Close the database connection\ndbDisconnect(con)\n</code></pre> <p>Why create a SQLite database instead of a tsv file?</p> <p>Creating a SQLite database offers several benefits over saving data in a tabular format:</p> <ol> <li>Efficient Querying: SQLite databases are optimized for querying and retrieving data. Complex queries and filtering operations are faster and more efficient, making it easier to extract specific information from the dataset.</li> <li>Structured Organization: Databases provide a structured and organized way to store data. Tables, relationships, and indices ensure that data remains well-organized and easily accessible.</li> <li>Data Integrity: SQLite databases enforce data integrity through constraints and validations. This helps prevent errors and inconsistencies in the data, ensuring high-quality and reliable information.</li> <li>Concurrency and Multi-User Support: SQLite databases support concurrent read access from multiple users, ensuring that data remains accessible while maintaining data integrity.</li> <li>Scalability: As datasets grow, SQLite databases continue to perform efficiently. They can handle larger datasets without significant degradation in performance.</li> <li>Modularity and Portability: Databases are self-contained and modular, meaning that a single database file can contain multiple tables, simplifying data distribution.</li> <li>Security and Access Control: SQLite databases offer security features, such as password protection and encryption, to safeguard sensitive data. Access to specific tables or data can be controlled through user roles and permissions.</li> <li>Indexing: SQLite databases support indexing, which significantly speeds up data retrieval based on specific columns. This is especially advantageous for large datasets.</li> <li>Data Relationships: Databases allow the establishment of relationships between tables, facilitating the storage of complex and interconnected data. This could be very relevant if you would like to connect information about projects, assays and samples.</li> </ol>"},{"location":"08_database.html#making-a-catalog-browser","title":"Making a catalog browser","text":"<p>Designing a catalog browser for your NGS database can be accomplished using tools like Rshiny or a Panel python app. These frameworks offer user-friendly interfaces for navigating and interacting with your database, providing features for dynamic search, filtering, and visualization. They allow for an accessible and efficient way to explore the contents of your database, which will be great for your teammates and your group leader.</p> <p>Below we will see how to create a simple browser tool using Rshiny from both a tsv file and a SQLite database.</p>"},{"location":"08_database.html#from-a-tsv-file","title":"From a tsv file","text":"<pre><code>library(shiny)\nlibrary(DT)\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"TSV File Viewer\"),\n\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Choose a TSV file\", accept = c(\".tsv\"))\n    ),\n\n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n\n  data &lt;- reactive({\n    req(input$file)\n    read.delim(input$file$datapath, sep = \"\\t\")\n  })\n\n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n</code></pre>"},{"location":"08_database.html#from-a-sqlite-database","title":"From a SQLite database","text":"<pre><code>library(shiny)\nlibrary(DBI)\nlibrary(DT)\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"SQLite Database Viewer\"),\n\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"db_file\", \"Choose an SQLite Database\", accept = c(\".sqlite\")),\n      textInput(\"table_name\", \"Enter Table Name:\", value = \"\"),\n      actionButton(\"load_button\", \"Load Table\")\n    ),\n\n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output, session) {\n\n  con &lt;- reactive({\n    if (!is.null(input$db_file)) {\n      dbConnect(SQLite(), input$db_file$datapath)\n    }\n  })\n\n  data &lt;- reactive({\n    req(input$load_button &gt; 0, input$table_name, con())\n    query &lt;- glue::glue_sql(\"SELECT * FROM {dbQuoteIdentifier(con(), input$table_name)}\")\n    dbGetQuery(con(), query)\n  })\n\n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n\n  observeEvent(input$load_button, {\n    output$table &lt;- renderDT({\n      datatable(data())\n    })\n  })\n\n  # Disconnect from the database when app closes\n  observe({\n    on.exit(dbDisconnect(con()), add = TRUE)\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n</code></pre>"},{"location":"08_database.html#see-an-example","title":"See an example","text":"<p>Below we have added an example of a SQLite database catalog created by the Brickman Lab at the Center for Stem Cell Medicine. Simple, but effective! As an additional feature, the Brickman lab has added the option to open the metadata.yml file whenever you click on a data row, allowing you to see the full extent of the metadata for that <code>Assay</code>.</p> <p></p>"},{"location":"08_database.html#future-ideas","title":"Future ideas","text":"<p>If you would like to make a more complex catalog browser, here are some ideas!</p> <ul> <li>Add a tab or an option where you can create an <code>Assay</code> or <code>Project</code> folder interactively from there as well as fill up metadata .</li> <li>Modify and correct existing entries</li> <li>Visualize an analysed single cell RNAseq dataset by opening Cirrocumulus session.</li> </ul> <p>Nonetheless, implementing these ideas are beyond the scope of this course, it would be very complicated to implement!</p> <p>Exercise: create a catalog of your NGS folders</p> <ol> <li>Create a folder call <code>Assays</code></li> <li>Under that folder, make three new <code>Assay</code> folders from your cookiecutter template</li> <li>Run the script above with R to create a database on a tsv file (or create your own with python). Modify the <code>folder_path</code> variable so it matches the path tot the folder <code>Assays</code>. The table will be written under the same <code>folder_path</code>.</li> <li>Visualize your <code>Assays</code> table with Excel</li> </ol>"},{"location":"08_database.html#wrap-up","title":"Wrap up","text":"<p>This lesson has been a very practical one. We have seen how useful your metadata can be! By collecting all the metadata in your folders, you are able to create a catalog that can be interactively explored using a Rshiny or Python panel tool. This allows you, your workmates and your group leader to keep track of all the experiments performed at the lab, making sure they will not be lost or forgotten. You will be able to see who made them, when and other interesting features that might help you understand the data at hand. In the next lesson, we will learn how to use version control tools in order to keep track of your data analyses and results using Git and GitHub.</p>"},{"location":"09_version_control.html","title":"Data analysis version control","text":""},{"location":"09_version_control.html#version-control-of-your-data-analysis-using-git-and-github","title":"Version control of your data analysis using Git and Github","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Learn what is version control and why it is important</li> <li>Introduce you to Git and Github repositories</li> <li>Make your own repositories</li> <li>Make a Github page to show off your data analysis reports</li> </ol> <p>In this lesson, we will explore the concept of version control and its role in modern research and development workflows. We'll introduce Git, a widely adopted version control system that allows you to systematically track changes in your work. Additionally, we will check GitHub, a collaborative platform for hosting Git repositories, enabling you to share your work with others and increase the visibility of your projects. By the end of this lesson, you'll have a introductory understanding of version control, the ability to create Git repositories, and the skills to build a GitHub page to showcase your data analysis.</p>"},{"location":"09_version_control.html#version-control","title":"Version control","text":"<p>Version control is a systematic approach to tracking changes made to a project over time. It provides a structured means of documenting alterations, allowing you to revisit and understand the evolution of your work. In research data management and data analytics, version control is very important and gives you a lot of advantages.</p> <p>Advantages of using version control</p> <ol> <li>Document Progress: Version control keeps a detailed history of changes, making it easier to understand how a project has developed, what modifications were made, and by whom.</li> <li>Ensure Data Integrity: It safeguards data by preventing accidental overwrites or deletions. Each change is tracked, enabling easy recovery in case of errors.</li> <li>Facilitate Collaboration: In collaborative research, version control enables multiple team members to work simultaneously on a project without conflicts. Changes can be merged seamlessly.</li> <li>Reproducibility: It enhances reproducibility by preserving the exact state of your project at any point in time. This is invaluable for validating research findings and data analysis.</li> <li>Branching and Experimentation: Version control allows for branching, where you can create alternative versions of your project for experimentation without affecting the main branch.</li> <li>Global Accessibility: Platforms like GitHub provide global visibility, allowing researchers to share their work, receive feedback, and contribute to open science.</li> </ol>"},{"location":"09_version_control.html#git-and-github","title":"Git and Github","text":"<p>Warning</p> <p>In this section we will only talk briefly about what is Git and Github. Explaining how git works is beyond the scope of this course. If you want to know more, please check out our course! You can also check GitHub documentation, which cover all the basics to work with Git and GitHub.</p>"},{"location":"09_version_control.html#version-control-using-git","title":"Version control using Git","text":"<p>Git is a distributed version control system that enables developers and researchers to efficiently manage their project's history, collaborate seamlessly, and ensure data integrity. At its core, Git operates through the following principles and mechanisms:</p> <ol> <li>Local Repository: Each user maintains a local repository on their computer, allowing them to work on a project independently. This local repository stores the complete project history.</li> <li>Snapshots, Not Files: Git does not track individual file changes but rather captures snapshots of the entire project at different points in time. This 'snapshot' approach ensures data consistency.</li> <li>Commits: Users create 'commits,' which are snapshots of the project at a specific moment. Commits record changes made to files, along with a commit message explaining the modifications.</li> <li>Branching: Git supports branching, enabling users to create separate lines of development. Branches are useful for experimenting with new features or fixing bugs without affecting the main project.</li> <li>Merging: Changes made in one branch can be merged into another, allowing for the incorporation of new features or bug fixes back into the main project. Git ensures a smooth merging process.</li> <li>Distributed Architecture: Git's distributed nature means that each user's local repository is a complete copy of the project, including its entire history. This enables offline work and ensures data redundancy.</li> <li>Remote Repositories: Git allows users to connect and synchronize their local repositories with remote repositories hosted on platforms like GitHub. Remote repositories facilitate collaboration and provide a central hub for project sharing.</li> <li>Push and Pull: Users 'push' their local changes to a remote repository to share with others. Conversely, they 'pull' changes made by others into their local repository to stay up-to-date.</li> <li>Conflict Resolution: In cases of conflicting changes, Git provides tools to resolve conflicts manually, ensuring that data integrity is maintained during collaboration.</li> <li>Versioning and Tagging: Git offers versioning and tagging capabilities, allowing users to mark specific points in history, such as major releases or significant milestones.</li> </ol>"},{"location":"09_version_control.html#hosting-your-git-repos-using-github","title":"Hosting your git repos using Github","text":"<p>On the other hand, GitHub is a web-based platform that enhances Git's capabilities by providing a collaborative and centralized hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allows you to create websites to showcase your projects. We will see more about Github Pages in the section below. In addition, you can set your repository as private until you are ready to publish your work!</p> <p>If you do not have an account in Github already, we recommend you to do one now! There are other alternatives to GitHub, such as BitBucket and GitLab, Some features might be different, so we will stick to Github for the rest of the lesson!</p>"},{"location":"09_version_control.html#creating-a-repo-from-your-project-folder","title":"Creating a repo from your Project folder","text":"<p>We will show you two ways to make your <code>Project</code> folder into a Git repository</p> <p>What about my Assay folder?</p> <p>An assay folder contains very big files that are not suitable for version control, at least in GitHub. We recommend that you deposit the data into a domain specific archive such as GEO or ArrayExpress/Annotare. Even Zenodo would be a better option in this case. We will look at them in the next session.</p> <p>Remember to link the original dataset with the data analysis repository!</p>"},{"location":"09_version_control.html#creating-a-git-repo-from-an-existing-folder","title":"Creating a git repo from an existing folder","text":"<p>Using <code>git init</code> is probably the right choice for you if you have created a <code>Project</code> folder using the cookiecutter template we saw in the previous lesson.</p> <p>The <code>git init</code> command should only be run once, even if other collaborators share will the project!</p> <ul> <li>First, initialize the repository (<code>git init</code>) and make at least one commit (<code>git add</code> and <code>git commit</code>).</li> <li>Once you have initialized the repository, create a remote repository in GitHub</li> <li>Then, add the remote URL to your local git repository with <code>git remote add origin \\&lt;URL\\&gt;</code>. This stores the remote URL under a more human-friendly name, origin.</li> <li>Shape your history into at least one commit by using <code>git add</code> to stage the existing files, and <code>git commit</code> to make the snapshot.</li> <li>Once you have at least one commit, you can push to the remote and set up the tracking relationship for good with <code>git push -u origin master</code>.</li> </ul>"},{"location":"09_version_control.html#creating-a-git-repo-online-and-copying-your-project-folder","title":"Creating a git repo online and copying your project folder","text":"<p>If the repository already exists on a remote, you would choose to <code>git clone</code> and not <code>git init</code>. On the other hand, if you create a remote repository first with the intent of moving your project to it later, you may have a few other steps to follow. If there are no commits in the remote repository, you can follow the steps above for <code>git init</code>. If there are commits and files in the remote repository but you would still like it to contain your project files, <code>git clone</code> that repository. Then, move the project's files into that cloned repository. <code>git add</code>, <code>git commit</code>, and <code>git push</code> to create a history that makes sense for the beginning of your project. Then, your team can interact with the repository without <code>git init</code> again.</p> <p>Tips to write good commit messages</p> <p>If you would like to know more about Git commits and the best way to make clear git messages, check out this post!</p>"},{"location":"09_version_control.html#github-pages","title":"GitHub Pages","text":"<p>Once you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, Rmarkdowns or html reports, in a GitHub Page website. Creating a GitHub page is very simple, and we really recommend that you follow the nice tutorial that GitHub as put for you.</p> <p>There are many different ways to create your webpages. We recommend using Mkdocs and Mkdocs materials as a framework to create a nice webpage in a simple manner. The folder templates that we used as an example in lesson 06 already contain everything you need to start a webpage. Nonetheless, you will need to understand the basics of MkDocs and MkDocs materials to design a webpage to your liking. MkDocs is a static webpage generator that is very easy to use, while MkDocs materials is an extension of the tool that gives you many more options to customize your website. Check out their webpages to get started!</p>"},{"location":"09_version_control.html#a-full-setup-example","title":"A full setup example","text":"<p>In this section we will showcase a full example about how to setup Git, MkDocs and a github account so you can do it yourself!</p>"},{"location":"09_version_control.html#install-all-required-tools-and-software","title":"Install all required tools and software","text":"<p>There are several tools and softwares that you need to install. First you will need <code>pip</code> and install the following packages using pip in the command line:</p> <pre><code>pip install cookiecutter # cookiecutter to create folder templates\npip install cruft # cruft is used to version control your templates\npip install mkdocs # mkdocs to create your webpages\npip install mkdocs-material # mkdocs extension to customize your templates\npip install mkdocs-video # mkdocs extension to add videos or embed internet videos, like youtube, to your webpages\npip install mkdocs-bibtex # mkdocs extension to add references in your text from a bib file\npip install neoteroi-mkdocs # mkdocs extension to create author cards\npip install mkdocs-minify-plugin # mkdocs extension to minimize the html code created by mkdocs\npip install mkdocs-git-revision-date-localized-plugin  # mkdocs extension to show \"last updated\" date of your webpage\npip install mkdocs-jupyter # mkdocs extension to include jupyter notebooks without needing to convert them\npip install mkdocs-table-reader-plugin # mkdocs extension to embed tabular format files like tsv or csv\n</code></pre> <p>Lastly and very importantly, install Git from their webpage.</p>"},{"location":"09_version_control.html#create-your-own-github-account","title":"Create your own GitHub account","text":"<p>Go to Github and create a new user.</p>"},{"location":"09_version_control.html#create-a-github-organization-for-your-lab-or-department","title":"Create a GitHub organization for your lab or department","text":"<p>GitHub allows users to create organizations and teams that will collaborate together or create repositories under the same umbrella organization. If you would like to create an educational organization in GitHub, you can do so for free! For example, you could create a GitHub account for your lab.</p> <p>In order to create a GitHub organization, follow these instructions</p> <p>After you have created the GitHub organization, make sure that you create your repositories under the organization space and not your own user!</p>"},{"location":"09_version_control.html#configure-your-main-github-page-and-its-repo","title":"Configure your main GitHub Page and its repo","text":"<p>The next step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps.</p> <p>After you have created the *organization*github.io, it is time to configure your webpage using MkDocs!</p>"},{"location":"09_version_control.html#use-mkdocs-to-create-your-webpage","title":"Use mkdocs to create your webpage","text":"<p>Follow the steps on the MkDocs documentation to get started on your webpage! You can use a simple markdown file describing your organization (your lab or department), its main goals and missions and maybe a couple of images showcasing your research.</p> <p>When you are happy with your webpage and are ready too publish it, make sure to add, commit and push the changes to the remote! Instead of using the basic setup that GitHub offers, we recommend that you build up your webpage using MkDocs and the <code>mkdocs gh-deploy</code> command! This requires a couple of changes in your GitHub organization settings.</p>"},{"location":"09_version_control.html#publishing-your-github-page","title":"Publishing your GitHub Page","text":"<p>Go to your GitHub organization settings and configure the Page section. Since you are using the <code>mkdocs gh-deploy</code> command to publish your site in the <code>gh-pages</code> branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website from:</p> <p></p> <ul> <li>Branch should be <code>gh-pages</code></li> <li>Folder should be <code>root</code></li> </ul> <p>After a couple of minutes, your webpage should be ready!</p>"},{"location":"09_version_control.html#make-a-repo-for-your-cookiecutter-template","title":"Make a repo for your cookiecutter template","text":"<p>Your GitHub organization account and webpage is ready! Now it is time to create a cookiecutter template for your folders using what you learned in this lesson.</p>"},{"location":"09_version_control.html#start-a-new-project-from-cookiecutter","title":"Start a new project from cookiecutter","text":"<p>Using cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.</p> <p>Next, link your data of interest and make an example of data analysis notebook/report. Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using <code>pip</code> allows you to directly add a Jupyter Notebook file to the <code>mkdocs.yml</code> navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.</p>"},{"location":"09_version_control.html#publishing-your-project-as-a-github-page","title":"Publishing your project as a GitHub Page","text":"<p>Remember to make sure that your markdowns, images, reports, etc., are included in the <code>docs</code> folder and properly set up in the navigation section of your <code>mkdocs.yml</code> file.</p> <p>Git add, commit and push your changes. Then, run <code>mkdocs gh-deploy</code>. You will still need to configure the settings of this repositories in GitHub, so that the Page is taken from the <code>gh-pages</code> branch and the <code>root</code> folder. You should be able to see your webpage through the link provided in the Page section!</p> <p>Now it is also possible to include this repository webpage in your main webpage organization*github.io by including the link of the repo website (https://*organization*github.io/*repo-name) in the navigation section of the <code>mkdocs.yml</code> file in the main *organization*github.io repo.</p> <p>Exercise 5: make a project folder and publish a data analysis webpage</p> <ol> <li>Configure your main GitHub Page and its repo</li> </ol> <p>The first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps. After you have created the *organization/username*github.io, it is time to configure your <code>Project</code> repository webpage using MkDocs!</p> <ol> <li>Start a new project from cookiecutter or use one from the previous exercise.</li> </ol> <p>If you use a <code>Project</code> repo from the first exercise, go to the next paragraph. Using cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.</p> <p>Next, link your data of interest (or create a small fake dataset) and make an example of data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using <code>pip</code> allows you to directly add a Jupyter Notebook file to the <code>mkdocs.yml</code> navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.</p> <p>For the purposes of this exercise, we have already included a basic <code>index.md</code> markdown file that can serve as the intro page of your repo, and a <code>jupyter_example.ipynb</code> with some code in it. You are welcome to modify them further to test them out!</p> <ol> <li>Use MkDocs to create your webpage</li> </ol> <p>When you are happy with your files and are ready too publish them, make sure to add, commit and push the changes to the remote. Then, build up your webpage using MkDocs and the <code>mkdocs gh-deploy</code> command from the same directory where the <code>mkdocs.yml</code> file is. For example, if your <code>mkdocs.yml</code> for your <code>Project</code> folder is in <code>/Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml</code>, do <code>cd /Users/JARH/Projects/project1_JARH_20231010/</code> and then <code>mkdocs gh-deploy</code>.</p> <p>Finally, we only need to set up the GitHub <code>Project</code> repo settings.</p> <ol> <li>Publishing your GitHub Page</li> </ol> <p>Go to your GitHub repo settings and configure the Page section. Since you are using the <code>mkdocs gh-deploy</code> command to publish your site in the <code>gh-pages</code> branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website from:</p> <p></p> <ul> <li>Branch should be <code>gh-pages</code></li> <li>Folder should be <code>root</code></li> </ul> <p>After a couple of minutes, your webpage should be ready!</p>"},{"location":"09_version_control.html#wrap-up","title":"Wrap up","text":"<p>In this lesson we have learned about version control and how we can use Git and GitHub to create data analyses repositories from your <code>Project</code> folders. We have also seen how one can create a GitHub organization and use GitHub Pages to display your data analyses scripts and notebooks for the general public! In the next lesson we will learn about where to archive the raw NGS data (and its metadata), as well as archiving your GitHub repositories.</p>"},{"location":"10_repos.html","title":"Repositories for NGS","text":""},{"location":"10_repos.html#repositories-for-ngs-data","title":"Repositories for NGS data","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: X minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Learn what are repositories and which ones are relevant for NGS data</li> <li>Learn where to archive your GitHub data analysis repositories</li> </ol> <p>In this lesson, we're going to explore repositories like Zenodo, Gene Expression Omnibus, and Annotare. While platforms like GitHub are great for version control and collaborative coding, these repositories serve a different purpose. They're designed specifically for archiving and sharing scientific data, ensuring it's preserved for the long term and accessible to the global research community. You could think of them as secure digital libraries for your valuable NGS data.</p>"},{"location":"10_repos.html#what-is-a-repositoryarchive","title":"What is a repository/archive?","text":"<p>Specialized repositories/archives are dedicated digital platforms designed for the secure storage, curation, and dissemination of scientific data. These repositories hold great importance in the research community as they serve as reliable archives for preserving valuable datasets. Their standardized formats and robust curation processes ensure the long-term accessibility and citability of research findings. Researchers worldwide rely on these repositories to share, discover, and validate scientific information, thereby fostering transparency, collaboration, and the advancement of knowledge across various domains of study.</p>"},{"location":"10_repos.html#why-are-they-important","title":"Why are they important?","text":"<p>Archiving your data in these repositories offers a multitude of benefits. Firstly, it ensures the enduring accessibility and preservation of your research, safeguarding it for future generations of scientists. Additionally, repositories typically provide a unique Identifier (often a DOI) for your dataset, granting it a citable status in the academic world. This not only enhances the visibility and impact of your work but also facilitates proper attribution.</p> <p>Moreover, these repositories often encourage or require comprehensive metadata to accompany your data. This rich contextual information includes details about the methodology, experimental setup, and any other pertinent information. Such metadata greatly enhances the discoverability and interpretability of your dataset, enabling fellow researchers to effectively use and build upon your work. In essence, archiving your data in these repositories not only fulfills scholarly obligations but also amplifies the reach and influence of your research in the scientific community.</p> <p>In addition, depositing data in these archives is often a mandatory requirement set by scientific journals and funding agencies. This reflects the growing recognition of the critical role these repositories play in ensuring transparency, reproducibility, and the integrity of research outcomes. By adhering to these guidelines, researchers contribute to the broader scientific community and increase the quality of their research.</p>"},{"location":"10_repos.html#types-of-repositories","title":"Types of repositories","text":"<p>We could divide archives in two mayor categories, general or domain-specific. General repositories like Zenodo cater to a wide range of disciplines, providing a versatile platform for data archiving. On the other hand, domain-specific repositories are tailored to particular scientific fields, like GEO and Annotare in the field of NGS, offering specialized curation and context-specific features. Utilizing domain-specific repositories often provides researchers with a more targeted audience, deeper domain expertise, and enhanced visibility within their specific research community. This focused approach ensures that data is curated and contextualized in a way that aligns closely with the standards and expectations of that particular field, maximizing its impact and utility.</p>"},{"location":"10_repos.html#domain-specific-repositories-for-ngs-data-geo-or-annotare","title":"Domain specific repositories for NGS data: GEO or Annotare","text":""},{"location":"10_repos.html#gene-expression-omnibus","title":"Gene Expression Omnibus","text":"<p>The Gene Expression Omnibus, commonly known as GEO, is a specialized repository curated by the National Center for Biotechnology Information (NCBI). It is dedicated to archiving and sharing high-throughput functional genomic data sets, primarily focused on gene expression data.</p> <p>Researchers can easily deposit and access a wide range of genomic data, including microarray and high-throughput sequencing studies. GEO provides a structured platform for researchers to share their findings with the scientific community, enhancing data transparency and reproducibility.</p> <p>GEO assigns unique accession numbers to each dataset, ensuring traceability and enabling proper citation in research publications. Its domain-specific focus on functional genomics makes it an invaluable resource for researchers in genetics, genomics, and related fields, allowing for the comprehensive exploration of gene expression patterns across various biological conditions and experimental designs.</p>"},{"location":"10_repos.html#annotare","title":"Annotare","text":"<p>Annotare is a specialized repository hosted by the European Bioinformatics Institute (EBI) that is tailored for the submission and storage of functional genomics experiments, particularly those involving high-throughput sequencing data. Unlike general repositories, Annotare provides a domain-specific platform optimized for researchers in the field of functional genomics.</p> <p>Researchers can upload their experimental data along with comprehensive metadata, ensuring that the context and details of the experiment are preserved. Annotare supports a wide range of genomic data types, making it a versatile platform for archiving diverse functional genomics studies.</p> <p>By focusing specifically on functional genomics, Annotare offers researchers a curated environment that aligns closely with the standards and practices of this specialized field. This ensures that data is stored and curated in a manner that is most useful for researchers in the genomics community. The platform's specialization enhances data discoverability, promotes collaboration, and facilitates deeper insights into the functional aspects of the genome.</p>"},{"location":"10_repos.html#ngs-data-upload-to-geo-or-annotare","title":"NGS Data upload to GEO or Annotare","text":"<p>GEO and Annotare are excellent repository choices to deposit your NGS data. Both Annotare and GEO adhere to established community standards for data submission and sharing in the field of functional genomics:</p> <ol> <li>Minimum Information About a Microarray Experiment (MIAME): This is a set of guidelines established to ensure the comprehensive and standardized reporting of microarray experiments. Both Annotare and GEO require compliance with MIAME standards for microarray data submissions.</li> <li>Minimum Information about a high-throughput SeQuencing Experiment (MIxS): MIxS is a set of standards developed by the Genomic Standards Consortium to ensure consistent reporting of metadata for high-throughput sequencing experiments. Annotare and GEO require adherence to MIxS standards for sequencing data submissions.</li> <li>Sequence Read Archive (SRA) Submission Guidelines: Both Annotare and GEO follow the submission guidelines set forth by the Sequence Read Archive, which include requirements for data formatting, metadata inclusion, and quality control.</li> <li>Community-Specific Standards: In addition to the above, Annotare and GEO may also adhere to community-specific standards and guidelines established by the functional genomics research community. These standards are designed to ensure that submitted data meets the specific requirements and expectations of the field.</li> </ol> <p>By adhering to these standards, Annotare and GEO ensure that the data submitted to their repositories is of high quality, well-documented, and compliant with community best practices. This facilitates data discovery, reproducibility, and interoperability within the scientific community.</p> <p>These repositories will only accept NGS data and information related to the creation of the data. This includes the raw FASTQ files, sample metadata, including protocols and descriptions of how the samples and data where processed, as well as final pre-processing results such as read count matrices or genomic position files (like BED). If you adhere to the <code>Assay</code> folder creation guideline of lesson 6, you will have a very easy time filling up the required documentation and information needed to submit the data in your <code>Assay</code> folder to one of these repositories.</p> <p>Nonetheless, the repositories will not accept other data created by your down-stream analyses, neither the code used for data analyses! This means anything that you have done in your <code>Project</code> folder. However, your <code>Project</code> folder is already version controlled by GitHub (see previous lesson), so there is no need to worry. We will see in the section below how to archive your `Project`` folder as well using a general repository like Zenodo.</p>"},{"location":"10_repos.html#a-general-repository-zenodo","title":"A general repository: Zenodo","text":"<p>Zenodo[https://zenodo.org/] is an open-access digital repository designed to facilitate the archiving of scientific research outputs. It operates under the umbrella of the European Organization for Nuclear Research (CERN) and is supported by the European Commission. Zenodo accommodates a broad spectrum of research outputs, including datasets, papers, software, and multimedia files. This versatility makes it an invaluable resource for researchers across a wide array of domains, promoting transparency, collaboration, and the advancement of knowledge on a global scale.</p> <p>Operating on a user-friendly web platform, Zenodo allows researchers to easily upload, share, and preserve their research data and related materials. Upon deposit, each item is assigned a unique Digital Object Identifier (DOI), granting it a citable status and ensuring its long-term accessibility. Additionally, Zenodo provides robust metadata capabilities, enabling researchers to enrich their submissions with detailed contextual information. In addition, it allows you to link you GitHub account, providing a streamlined way to archive a specific release of your GitHub repository directly into Zenodo. This integration simplifies the process of preserving a snapshot of your project's progress for long-term accessibility and citation.</p>"},{"location":"10_repos.html#project-archiving-in-zenodo","title":"<code>Project</code> archiving in Zenodo","text":"<p>Once your accounts are linked, creating a Zenodo archive is as simple as tagging a release in your GitHub repository. Zenodo will automatically detect the release and generate a corresponding archive. This archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work. So, before submitting your work in a journal, make sure to link your data analysis repository to Zenodo, get a DOI and cite it in your manuscript!</p> <p>By leveraging this integration, you ensure that significant milestones in your project are preserved in a reliable and accessible manner. This not only facilitates proper attribution but also contributes to the broader scientific community's ability to reproduce and build upon your research.</p> <p>Exercise 6: Archive a <code>Project</code> GitHub repo in Zenodo</p> <ol> <li>In order to archive your GitHub repos in Zenodo, you will first need to link your Zenodo and GitHub accounts.</li> <li>Once your accounts are linked, go to your Zenodo GitHub account settings and turn on the GitHub repository you want to archive. </li> <li>Creating a Zenodo archive is now as simple as making a release in your GitHub repository. Remember to make a proper tag! NOTE: If you make a release before enabling the GitHub repository in Zenodo, it will not appear in Zenodo! </li> <li>Zenodo will automatically detect the release and it should appear in your Zenodo upload page. </li> <li>This archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work. </li> </ol> <p>Before submitting your work in a journal, make sure to link your data analysis repository to Zenodo, get a DOI and cite it in your manuscript!</p>"},{"location":"10_repos.html#wrap-up","title":"Wrap up","text":"<p>In this final lesson we have learned how to wrap up a project/manuscript experiment by submitting your data to a domain-specific repository, while archiving your data analysis GitHub repositories in Zenodo. By following the simple lessons shown in this workshop, you will dramatically improve the FAIRability of your data, as well as organizing and structuring it in a way that will be much more useful in the future. This advantages do not serve yourself only, but your teammates, group leader and the general scientific population!</p> <p>We hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to answer this form! </p> <p>FEEDBACK FORM</p>"},{"location":"contributors.html","title":"Contributors","text":"<p>Jose Alejandro Romero Herrera  </p>","tags":["authors","contributors"]},{"location":"contributors.html#credit-table","title":"CRediT table","text":"CRediT role Initials Conceptualization JARH Data curation JARH Formal Analysis JARH Funding acquisition JARH Investigation JARH Methodology JARH Project administration JARH Resources JARH Software JARH Supervision JARH Validation JARH Visualization JARH Writing - original draft JARH Writing - review &amp; editing JARH","tags":["authors","contributors"]},{"location":"keywords.html","title":"Keyword index","text":""},{"location":"keywords.html#keywords","title":"Keywords","text":"<p>Here's a lit of used keywords:</p>"},{"location":"keywords.html#bad-rdm","title":"Bad RDM","text":"<ul> <li>RDM intro</li> </ul>"},{"location":"keywords.html#bioinformatics-data","title":"Bioinformatics data","text":"<ul> <li>Next Generation Sequencing Data</li> </ul>"},{"location":"keywords.html#dmp","title":"DMP","text":"<ul> <li>Data Management Plans</li> </ul>"},{"location":"keywords.html#dtu-workshop","title":"DTU workshop","text":"<ul> <li>DTU workshop 2023</li> </ul>"},{"location":"keywords.html#data-life-cycle","title":"Data Life Cycle","text":"<ul> <li>Data Life Cycle</li> </ul>"},{"location":"keywords.html#data-management-plan","title":"Data Management Plan","text":"<ul> <li>Data Management Plans</li> </ul>"},{"location":"keywords.html#data-formats","title":"Data formats","text":"<ul> <li>Next Generation Sequencing Data</li> </ul>"},{"location":"keywords.html#exercises","title":"Exercises","text":"<ul> <li>DTU workshop 2023</li> </ul>"},{"location":"keywords.html#file-organization","title":"File organization","text":"<ul> <li>File structure and naming conventions</li> </ul>"},{"location":"keywords.html#file-structure","title":"File structure","text":"<ul> <li>File structure and naming conventions</li> </ul>"},{"location":"keywords.html#folder-templates","title":"Folder templates","text":"<ul> <li>File structure and naming conventions</li> </ul>"},{"location":"keywords.html#good-rdm","title":"Good RDM","text":"<ul> <li>RDM intro</li> </ul>"},{"location":"keywords.html#ngs","title":"NGS","text":"<ul> <li>Next Generation Sequencing Data</li> </ul>"},{"location":"keywords.html#naming-conventions","title":"Naming conventions","text":"<ul> <li>File structure and naming conventions</li> </ul>"},{"location":"keywords.html#practical-lessons","title":"Practical lessons","text":"<ul> <li>DTU workshop 2023</li> </ul>"},{"location":"keywords.html#rdm","title":"RDM","text":"<ul> <li>RDM intro</li> </ul>"},{"location":"keywords.html#authors","title":"authors","text":"<ul> <li>Contributors</li> </ul>"},{"location":"keywords.html#contributors","title":"contributors","text":"<ul> <li>Contributors</li> </ul>"},{"location":"practical_workshop.html","title":"DTU workshop 2023","text":"","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#dtu-workshop-october-2023","title":"DTU workshop October 2023","text":"<p>Last updated: November 30, 2023</p> <p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: 75 minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Organize and structure your data and data analysis with cookiecutter templates</li> <li>Establish metadata fields and collect metadata when creating a cookiecutter folder</li> <li>Establish naming conventions for your data</li> <li>Make a catalog of your data</li> <li>Create GitHub repositories of your data analysis and display them as GitHub Pages</li> <li>Archive GitHub repositories on Zenodo</li> </ol> <p>This is a practical version of the full RDM on NGS data workshop. The main key points of the exercises shown here is to help you organize and structure your NGS datasets and your data analyses. We will see how to keep track of your experiments metadata and how to safely version control and archive your data analyses using GitHub repositories and Zenodo. We hope that through these practical exercises and step-by-step guidance, you'll gain valuable skills in efficiently managing and sharing your research data, enhancing the reproducibility and impact of your work.</p> <p>Requirements</p> <p>In order to follow this tutorial you will need:</p> <ul> <li>A GitHub account and git installed in your computer.</li> <li>A Zenodo account</li> <li>Python and pip installed</li> <li>MkDocs and MkDocs material theme (can be installed through pip)</li> <li>Cookicutter (can be installed through pip)</li> </ul> <p>In addition, you should install the following MkDocs extensions</p> <pre><code>pip install mkdocs\npip install mkdocs-material\npip install mkdocs-minify-plugin\npip install mkdocs-git-revision-date-localized-plugin \npip install mkdocs-jupyter\npip install mkdocs-table-reader-plugin\n</code></pre>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#1-organize-and-structure-your-ngs-data-and-data-analysis","title":"1. Organize and structure your NGS data and data analysis","text":"<p>Applying a consistent file structure and naming conventions to your files will help you to efficiently manage your data. We will divide your NGS data and data analyses into two different types of folders:</p> <ol> <li>Assay folders: These folders contain the raw and processed NGS datasets, as well as the pipeline/workflow used to generate the processed data, provenance of the raw data and quality control reports of the data. This data should be locked and read-only to prevent unwanted modifications.</li> <li>Project folders: These folders contain all the necessary files for a specific research project. A project may use several assays or results from other projects. The assay data should not be copied or duplicated, but linked from the original source.</li> </ol> <p>Projects and Assays are separated from each other because a project may use one or more assays to answer a scientific question, and assays may be reused several times in different projects. This could be, for example, all the data analysis related to a publication (a RNAseq and a ChIPseq experiment), or a comparison between a previous ATACseq experiment (which was used for a older project) with a new laboratory protocol.</p> <p>You could also create Genomic resources folders such things such as genome references (fasta files) and annotations (gtf files) for different species, as well as indexes for different alignment algorithms. If you want to know more, feel free to check the relevant full lesson</p> <p>This will help you to keep your data tidied up, specially if you are working on a big lab where assays may be used for different purposes and different people!</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#assay-folder","title":"Assay folder","text":"<p>For each NGS experiment there should be an <code>Assay</code> folder that will contain all experimental datasets, that is, an <code>Assay</code> (raw files and pipeline processed files). Raw files should not be modified at all, but you should probably lock modifications to the final results once you are done with preprocessing the data. This will help you prevent unwanted modifications to the data. Each <code>Assay</code> subfolder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name an NGS assay using an acronym for the type of NGS assay (RNAseq, ChIPseq, ATACseq), a keyword that represents a unique descriptive element of that assay, and the date. Like this:</p> <pre><code>&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\n</code></pre> <p>For example <code>CHIP_Oct4_20230101</code> is a ChIPseq assay made on 1<sup>st</sup> January 2023 with the keyword Oct4, so it is easily identifiable by eye. Next, let's take a look at a possible folder structure and what kind of files you can find there.</p> <pre><code>CHIP_Oct4_20230101/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 metadata.yml\n\u251c\u2500\u2500 pipeline.md\n\u251c\u2500\u2500 processed\n\u2514\u2500\u2500 raw\n   \u251c\u2500\u2500 .fastq.gz\n   \u2514\u2500\u2500 samplesheet.csv\n</code></pre> <ul> <li>README.md: Long description of the assay in markdown format. It should contain provenance of the raw NGS data (samples, laboratory protocols used, aim of the assay, etc)</li> <li>metadata.yml: metadata file for the assay describing different keys and important information regarding that assay (see this lesson).</li> <li>pipeline.md: description of the pipeline used to process raw data, as well as the commands used to run the pipeline.</li> <li>processed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.</li> <li>raw: folder with the raw data.<ul> <li>.fastq.gz:In the case of NGS assays, there should be fastq files.</li> <li>samplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. You can also add extra columns with info regarding the experimental variables and batches so it can be used for downstream analysis as well.</li> </ul> </li> </ul>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#project-folder","title":"Project folder","text":"<p>On the other hand, we have the other type of folder called <code>Projects</code>. In this folder you will save a subfolder for each project that you (or your lab) works on. Each <code>Project</code> subfolder will contain project information and all the data analysis notebooks and scripts used in that project.</p> <p>As like for an Assay folder, the Project folder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name it after the main author initials, a keyword that represents a unique descriptive element of that assay, and the date:</p> <pre><code>&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n</code></pre> <p>For example, <code>JARH_Oct4_20230101</code>, is a project about the gene Oct4 owned by Jose Alejandro Romero Herrera, created on the 1<sup>st</sup> of January of 2023.</p> <p>Next, let's take a look at a possible folder structure and what kind of files you can find there.</p> <pre><code>&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n\u251c\u2500\u2500 data\n\u2502  \u2514\u2500\u2500 &lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD/\n\u251c\u2500\u2500 documents\n\u2502  \u2514\u2500\u2500 Non-sensitive_NGS_research_project_template.docx\n\u251c\u2500\u2500 notebooks\n\u2502  \u2514\u2500\u2500 01_data_analysis.rmd\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 reports\n\u2502  \u251c\u2500\u2500 figures\n\u2502  \u2502  \u2514\u2500\u2500 01_data_analysis/\n\u2502  \u2502   \u2514\u2500\u2500 heatmap_sampleCor_20230102.png\n\u2502  \u2514\u2500\u2500 01_data_analysis.html\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 01_data_analysis/\n\u2502      \u2514\u2500\u2500 DEA_treat-control_LFC1_p01.tsv\n\u251c\u2500\u2500 scripts\n\u2514\u2500\u2500 metadata.yml\n</code></pre> <ul> <li>data: folder that contains symlinks or shortcuts to where the data is, avoiding copying and modification of original files.</li> <li>documents: folder containing word documents, slides or pdfs related to the project, such as explanations of the data or project, papers, etc. It also contains your Data Management Plan.<ul> <li>Non-sensitive_NGS_research_project_template.docx. This is a pre-filled Data Management Plan based on the Horizon Europe guidelines.</li> </ul> </li> <li>notebooks: folder containing Jupyter, R markdown or Quarto notebooks with the actual data analysis.</li> <li>README.md: detailed description of the project in markdown format.</li> <li>reports: notebooks rendered as html/docx/pdf versions, ideal for sharing with colleagues and also as a formal report of the data analysis procedure.<ul> <li>figures: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.</li> </ul> </li> <li>requirements.txt: file explaining what software and libraries/packages and their versions are necessary to reproduce the code.</li> <li>results: results from the data analysis, such as tables with differentially expressed genes, enrichment results, etc.</li> <li>scripts: folder containing helper scripts needed to run data analysis or reproduce the work of the folder</li> <li>description.yml: short description of the project.</li> <li>metadata.yml: metadata file for the assay describing different keys (see this lesson).</li> </ul>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#template-engine","title":"Template engine","text":"<p>It is very easy to create a folder template using cookiecutter. Cookiecutter is a command-line utility that creates projects from cookiecutters (that is, a template), e.g. creating a Python package project from a Python package project template. Here you can find an example of a cookiecutter folder template directed to NGS data, where we have applied the structures explained in the previous sections. You are very welcome to adapt it or modify it to your needs!</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#quick-tutorial-on-cookiecutter","title":"Quick tutorial on cookiecutter","text":"<p>Creating a Cookiecutter template from scratch involves defining a folder structure, creating a <code>cookiecutter.json</code> file, and specifying the placeholders (keywords) that will be replaced during project generation. Let's walk through the process step by step:</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#step-1-create-a-folder-template","title":"Step 1: Create a Folder Template","text":"<p>Start by creating a folder with the structure you want for your template. For example, let's create a simple Python project template:</p> <pre><code>my_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\n</code></pre> <p>In this example, <code>{{cookiecutter.project_name}}</code> is a placeholder that will be replaced with the actual project name when the template is used.</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#step-2-create-cookiecutterjson","title":"Step 2: Create <code>cookiecutter.json</code>","text":"<p>In the root of your template folder, create a file named <code>cookiecutter.json</code>. This file will define the variables (keywords) that users will be prompted to fill in. For our Python project template, it might look like this:</p> <pre><code>{\n  \"project_name\": \"MyProject\",\n  \"author_name\": \"Your Name\",\n  \"description\": \"A short description of your project\"\n}\n</code></pre> <p>These are the questions users will be asked when generating a project based on your template. The values provided here will be used to replace the corresponding placeholders in the template files.</p> <p>In addition to replacing placeholders in file and directory names, Cookiecutter can also automatically fill in information within the contents of text files. This can be useful for providing default configurations or templates for code files. Let's extend our previous example to include a placeholder inside a text file:</p> <p>First, modify the <code>my_template/main.py</code> file to include a placeholder inside its contents:</p> <pre><code># main.py\n\ndef hello():\n    print(\"Hello, {{cookiecutter.project_name}}!\")\n</code></pre> <p>Now, the <code>{{cookiecutter.project_name}}</code> placeholder is inside the <code>main.py</code> file. When you run Cookiecutter, it will automatically replace the placeholders not only in file and directory names but also within the contents of text files. After running Cookiecutter, your generated <code>main.py</code> file might look like this:</p> <pre><code># main.py\n\ndef hello():\n    print(\"Hello, MyProject!\")  # Assuming \"MyProject\" was entered as the project_name\n</code></pre>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#step-3-use-cookiecutter","title":"Step 3: Use Cookiecutter","text":"<p>Now that your template is set up, you can use Cookiecutter to generate a project based on it. Open a terminal and run:</p> <pre><code>cookiecutter path/to/your/template\n</code></pre> <p>Cookiecutter will prompt you to fill in the values for <code>project_name</code>, <code>author_name</code>, and <code>description</code>. After you provide these values, Cookiecutter will replace the placeholders in your template files with the entered values.</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#step-4-explore-the-generated-project","title":"Step 4: Explore the Generated Project","text":"<p>Once the generation process is complete, navigate to the directory where Cookiecutter created the new project. You will see a project structure with the placeholders replaced by the values you provided.</p> <p>Exercise 1: Create your own template</p> <p>Using cookiecutter, create your own templates for your folders. You do not need to copy exactly our suggestions, adjust your template to your own needs!</p> <p>We have prepared already two simple cookiecutter templates in GitHub repositories.</p> <p>Assay</p> <ol> <li>First, fork our Assay folder template from the GitHub page into your own account/organization. </li> <li>Then, use <code>git clone &lt;your URL to the template&gt;</code> to put it in your computer.</li> <li>Modify the contents of the repository so that it matches the Assay example above. You are welcome to do changes as you please!</li> <li>Modify the <code>cookiecutter.json</code> file so that it will include the Assay name template</li> <li>Git add, commit and push your changes</li> <li>Test your folder by using <code>cookiecutter &lt;URL to your GitHub repository for \"assay-template&gt;</code></li> </ol> <p>Project</p> <ol> <li>First, fork our Project folder template from the GitHub page into your own account/organization. </li> <li>Then, use <code>git clone &lt;your URL to the template&gt;</code> to put it in your computer.</li> <li>Modify the contents of the repository so that it matches the Project example above. You are welcome to do changes as you please!</li> <li>Modify the <code>cookiecutter.json</code> file so that it will include the Project name template</li> <li>Git add, commit and push your changes</li> <li>Test your folder by using <code>cookiecutter &lt;URL to your GitHub repository for \"project-template&gt;</code></li> </ol>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#2-metadata-and-naming-conventions","title":"2. Metadata and naming conventions","text":"<p>Metadata is the behind-the-scenes information that makes sense of data and gives context and structure. For NGS data, metadata includes information such as when and where the data was collected, what it represents, and how it was processed. Let's check what kind of relevant metadata is available for NGS data and how to capture it in your Assay or Project folders. Both of these folders contain a metadata.yml file and a README.md file. In this section, we will check what kind of information you should collect in each of these files.</p> <p>Metadata and controlled vocabularies</p> <p>In order for metadata to be most useful, you should try to use controlled vocabularies for all your fields. For example, tissue could be described with the UBERON ontologies, species using the NCBI taxonomy, diseases using the Mondo database, etc. Unfortunately, implementing a systematic way of using these vocabularies is rather complex and outside the scope of this workshop, but you are very welcome to try to implement them on your own!</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#readmemd-file","title":"README.md file","text":"<p>The README.md file is a markdown file that allows you to write a long description of the data placed in a folder. Since it is a markdown file, you are able to write in rich text format (bold, italic, include links, etc) what is inside the folder, why it was created/collected, how and when. If it is an <code>Assay</code> folder, you could include the laboratory protocol used to generate the samples, images explaining the experiment design, a summary of the results of the experiment and any sort of comments that would help to understand the context of the experiment. On the other hand, a 'Project' README file may contain a description of the project, what are its aims, why is it important, what 'Assays' is it using, how to interpret the code notebooks, a summary of the results and, again, any sort of comments that would help to understand the project.</p> <p>Here is an example of a README file for a `Project`` folder:</p> <pre><code># NGS Analysis Project: Exploring Gene Expression in Human Tissues\n\n## Aims\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\n\n## Why It's Important\n\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n## Datasets\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\n\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n## Summary of Results\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\n\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n---\n\nFor more details, refer to our [Jupyter Notebook](link-to-jupyter-notebook.ipynb) for the complete analysis pipeline and code.\n</code></pre>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#metadatayml","title":"metadata.yml","text":"<p>The metadata file is a yml file, which is a text document that contains data formatted using a human-readable data format for data serialization.</p> <p></p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#metadata-fields","title":"Metadata fields","text":"<p>There is a ton of information you can collect regarding an NGS assay or a project. Some information fields are very general, such as author or date, while others are specific to the Assay or Project folder. Below, we will take a look at minimal information you should collect in each of the folders.</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#general-metadata-fields","title":"General metadata fields","text":"<p>Here you can find a list of suggestions for general metadata fields that can be used for both assays and project folders:</p> <ul> <li>Title: A brief yet informative name for the dataset.</li> <li>Author(s): The individual(s) or organization responsible for creating the dataset. You can use your ORCID</li> <li>Date Created: The date when the dataset was originally generated or compiled. Use YYYY-MM-DD format!</li> <li>Description: A short narrative explaining the content, purpose, and context.</li> <li>Keywords: A set of descriptive terms or phrases that capture the folder's main topics and attributes.</li> <li>Version: The version number or identifier for the folder, useful for tracking changes.</li> <li>License: The type of license or terms of use associated with the dataset/project.</li> </ul>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#assay-metadata-fields","title":"Assay metadata fields","text":"<p>Here you will find a table with possible metadata fields that you can use to annotate and track your <code>Assay</code> folders:</p> Metadata field Definition Format Ontology Example assay_ID Identifier for the assay that is at least unique within the project &lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD nan CHIP_Oct4_20200101 assay_type The type of experiment performed, eg ATAC-seq or seqFISH nan ontology field- e.g. EFO or OBI ChIPseq assay_subtype More specific type or assay like bulk nascent RNAseq or single cell ATACseq nan ontology field- e.g. EFO or OBI bulk ChIPseq owner Owner of the assay (who made the experiment?). &lt;First Name&gt; &lt;Last Name&gt; nan Jose Romero platform The type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform nan ontology field- e.g. EFO or OBI Illumina extraction_method Technique used to extract the nucleic acid from the cell nan ontology field- e.g. EFO or OBI nan library_method Technique used to amplify a cDNA library nan ontology field- e.g. EFO or OBI nan external_accessions Accession numbers from external resources to which assay or protocol information was submitted nan eg protocols.io, AE, GEO accession number, etc GSEXXXXX keyword Keyword for easy identification wordWord camelCase Oct4ChIP date Date of assay creation YYYYMMDD nan 20200101 nsamples Number of samples analyzed in this assay &lt;integer&gt; nan 9 is_paired Paired fastq files or not &lt;single OR paired&gt; nan single pipeline Pipeline used to process data and version nan nan nf-core/chipseq -r 1.0 strandedness The strandedness of the cDNA library &lt;+ OR - OR *&gt; nan * processed_by Who processed the data &lt;First Name&gt; &lt;Last Name&gt; nan Sarah Lundregan organism Organism origin &lt;Genus species&gt; Taxonomy name Mus musculus origin Is internal or external (from a public resources) data &lt;internal OR external&gt; nan internal path Path to files &lt;/path/to/file&gt; nan nan short_desc Short description of the assay plain text nan Oct4 ChIP after pERK activation ELN_ID ID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling plain text nan nan","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#project-metadata-fields","title":"Project metadata fields","text":"<p>Here you will find a table with possible metadata fields that you can use to annotate and track your <code>Project</code> folders:</p> Metadata field Definition Format Ontology Example project Project ID &lt;surname&gt;_et_al_2023 nan proks_et_al_2023 author Owner of the project &lt;First name&gt; &lt;Surname&gt; nan Martin Proks date Date of creation YYYYMMDD nan 20230101 description Short description of the project Plain text nan This is a project describing the effect of Oct4 perturbation after pERK activation","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#more-info","title":"More info","text":"<p>The information provided in this lesson is not at all exhaustive. There might be many more fields and controlled vocabularies that could be useful for your NGS data. We recommend that you take a look at the following sources for more information!</p> <ul> <li>Transcriptomics metadata standards and fields</li> <li>Bionty: Biological ontologies for data scientists.</li> </ul> <p>Exercise 2: modify the metadata.yml files in your cookiecutter templates</p> <p>We have seen some examples of metadata for NGS data. It is time now to customize your cookiecutter templates and modify the metadata.yml files so that they fit your needs! </p> <ol> <li>Think about what kind of metadata you would like to include.</li> <li>Modify the <code>cookiecutter.json</code> file so that when you create a new folder template, all the metadata is filled accordingly. </li> <li>Modify the <code>metadata.yml</code> file so that it includes the metadata recorded by the <code>cookiecutter.json</code> file. </li> <li>Modify the <code>README.md</code> file so that it includes the short description recorded by the <code>cookiecutter.json</code> file.</li> <li>Git add, commit and push the changes of your template.</li> <li>Test your folders by using the command <code>cookiecutter &lt;URL to your cookiecutter repository in GitHub&gt;</code></li> </ol>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#3-naming-conventions","title":"3. Naming conventions","text":"<p>Using consistent naming conventions is important in scientific research as it helps with the organization and retrieval of data or results. By adopting standardized naming conventions, researchers ensure that files, experiments, or data sets are labeled in a clear, logical manner. This makes it easier to locate and compare similar types of data or results, even when dealing with large datasets or multiple experiments. For instance, in genomics, employing uniform naming conventions for files related to specific experiments or samples allows for swift identification and comparison of relevant data, streamlining the research process and contributing to the reproducibility of findings. This practice promotes efficiency, collaboration, and the integrity of scientific work.</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#general-tips","title":"General tips","text":"<p>Below you will find a small list of general tips to follow when you name a folder or a file:</p> <ul> <li>Use only alphanumeric characters to write a word: a to z and 0 to 9</li> <li>Avoid special characters: ~!@#$%^&amp;*()`[]{}\"|</li> <li>Date format: use <code>YYYYMMDD</code> format. For example: 20230101.</li> <li>Authors: use initials. For example: JARH</li> <li>Don't use of spaces! Computers get very confused when you need to point a path to a file and it contains spaces! Instead:<ul> <li>Separate field sections are separated by underscores <code>_</code>.</li> <li>Words in each section are written in camelCase.   It would look then like this: <code>field1_word1Word2.txt</code>. For example: <code>heatmap_sampleCor_20230101.png</code>. The first field indicates what this file is, i.e., a heatmap. Second field is what is being plotted, i.e, sample correlations; since the field contains two words, they are written in camelCase. The third field is the date of when the image was created.</li> </ul> </li> <li>Use as short fields as possible. You can try to use understandable abbreviations, like LFC for LogFoldChange, Cor for correlations, Dist for distances, etc.</li> <li>Avoid long names as much as you can, be concise!</li> <li>Avoid creating many sublevels of folders.</li> <li>Write down your naming convention pattern and document it in the README file</li> <li>When using a sequential numbering system, use leading zeros to make sure files sort in sequential order. Using <code>01</code> instead of just <code>1</code> if your sequence only goes up to <code>99</code>.</li> <li>Versions should be used as the last element, and use at least two digits with a leading 0 (e.g. v01, v02)</li> </ul>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#suggestions-for-ngs-data","title":"Suggestions for NGS data","text":"<p>More info on naming conventions for different types of files and analysis is in development.</p> name description naming_convention file format example .fastq raw sequencing reads nan nan sampleID_run_read1.fastq .fastqc quality control from fastqc nan nan sampleID_run_read1.fastqc .bam aligned reads nan nan sampleID_run_read1.bam GTF sequence annotation nan nan one of https://www.gencodegenes.org/ GFF sequence annotation nan nan one of https://www.gencodegenes.org/ .bed genome locations nan nan nan .bigwig genome coverage nan nan nan .fasta sequence data (nucleotide/aminoacid) nan nan one of https://www.gencodegenes.org/ Multiqc report QC aggregated report &lt;assayID&gt;_YYYYMMDD.multiqc multiqc RNA_20200101.multiqc Count matrix final count matrix &lt;assayID&gt;_cm_aligner_YYYYMMDD.tsv tsv RNA_cm_salmon_20200101.tsv DEA differential expression analysis results DEA_&lt;condition1-condition2&gt;_LFC&lt;absolute_threshold&gt;_p&lt;pvalue decimals&gt;_YYYYMMDD.tsv tsv DEA_treat-untreat_LFC1_p01_20200101.tsv DBA differential binding analysis results DBA_&lt;condition1-condition2&gt;_LFC&lt;absolute_threshold&gt;_p&lt;pvalue decimals&gt;_YYYYMMDD.tsv tsv DBA_treat-untreat_LFC1_p01_20200101.tsv MAplot MA plot MAplot_&lt;condition1-condition2&gt;_YYYYMMDD.jpeg jpeg MAplot_treat-untreat_20200101.jpeg Heatmap plot Heatmap plot of anything heatmap_&lt;type&gt;_YYYYMMDD.jpeg jpeg heatmap_sampleCor_20200101.jpeg Volcano plot Volcano plot volcano_&lt;condition1-condition2&gt;_YYYYMMDD.jpeg jpeg volcano_treat-untreat_20200101.jpeg Venn diagram Venn diagram venn_&lt;type&gt;_YYYYMMDD.jpeg jpeg venn_consensus_20200101.jpeg Enrichment table Enrichment results nan tsv nan <p>Exercise 3: Create your own naming conventions</p> <p>Think about the most common types of files and folders you will be working on, such as visualizations, results tables, processed files, etc. Then come up with a logical and clear way of naming those files using the tips suggested above. Remember to avoid making long and complicated names!</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#4-create-a-catalog-of-your-assays-folder","title":"4. Create a catalog of your assays folder","text":"<p>The next step is to collect all the NGS datasets that you have created in the manner explained above. Since your folders all should contain the <code>metadata.yml</code> file in the same place with the same metadata, it should be very easy to iteratively go through all the folders and merge all the metadata.yml files into a one single table. This table can be then browsed easily with Microsoft Excel, for example. If you are interested in making a Shiny app or Python Panel tool to interactively browse the catalog, check out this lesson.</p> <p>Exercise 4: create a metadata.tsv catalog</p> <p>We will make a small script in R (or you can make one with python) that recursively goes through all the folders inside a input path (like your <code>Assays</code> folder), fetch all the <code>metadata.yml</code> files and merge them. Finally, it will write a tsv file as an output. </p> <ol> <li>Create a folder call <code>Assays</code></li> <li>Under that folder, make three new <code>Assay</code> folders from your cookiecutter template</li> <li>Run the script below with R (or create your own with python). Modify the <code>folder_path</code> variable so it matches the path tot the folder <code>Assays</code>. The table will be written under the same <code>folder_path</code>.</li> <li>Visualize your <code>Assays</code> table with Excel</li> </ol> <pre><code>library(yaml)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n    file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n    metadata_list &lt;- lapply(file_list, yaml::yaml.load_file)\n    return(metadata_list)\n    }\n\n# Specify the folder path\nfolder_path &lt;- \"/path/to/your/folder\"\n\n# Fetch metadata from the specified folder\nmetadata &lt;- get_metadata(folder_path)\n\n# Convert metadata to a data frame\nmetadata_df &lt;- data.frame(matrix(unlist(metadata), ncol = length(metadata), byrow = TRUE))\ncolnames(metadata_df) &lt;- names(metadata[[1]])\n\n# Save the data frame as a TSV file\noutput_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".tsv\")\nwrite.table(metadata_df, file = output_file, sep = \"\\t\", quote = FALSE, row.names = FALSE)\n\n# Print confirmation message\ncat(\"Database saved as\", output_file, \"\\n\")\n</code></pre>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#5-version-control-of-your-data-analysis-using-git-and-github","title":"5. Version control of your data analysis using Git and Github","text":"<p>Version control is a systematic approach to tracking changes made to a project over time. It provides a structured means of documenting alterations, allowing you to revisit and understand the evolution of your work. In research data management and data analytics, version control is very important and gives you a lot of advantages.</p> <p>Git is a distributed version control system that enables developers and researchers to efficiently manage their project's history, collaborate seamlessly, and ensure data integrity. At its core, Git operates through the following principles and mechanisms: On the other hand, GitHub is a web-based platform that enhances Git's capabilities by providing a collaborative and centralized hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allows you to create websites to showcase your projects.</p> <p>Create a GitHub organization for your lab or department</p> <p>GitHub allows users to create organizations and teams that will collaborate together or create repositories under the same umbrella organization. If you would like to create an educational organization in GitHub, you can do so for free! For example, you could create a GitHub account for your lab.</p> <p>In order to create a GitHub organization, follow these instructions</p> <p>After you have created the GitHub organization, make sure that you create your repositories under the organization space and not your own user!</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#creating-a-git-repo-online-and-copying-your-project-folder","title":"Creating a git repo online and copying your project folder","text":"<p>Version controlling your data analysis folders, a.k.a. <code>Project</code> folder, is very easy once you have set up your cookiecutter templates. The simplest way of doing this is to first create a remote GitHub repository from the webpage (or from the Desktop app, if you are using it) with a proper project name. Then <code>git clone</code> that repository you just made into your <code>Projects</code> main folder. Then, use cookiecutter to create a project folder template and copy-paste the contents of the folder template to your cloned repo. If you wish, you could already git add, commit and push the first changes to the folders and continue from there on.</p> <p>Tips to write good commit messages</p> <p>If you would like to know more about Git commits and the best way to make clear git messages, check out this post!</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#github-pages","title":"GitHub Pages","text":"<p>Once you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, Rmarkdowns or html reports, in a GitHub Page website. Creating a GitHub page is very simple, and we really recommend that you follow the nice tutorial that GitHub as put for you. Nonetheless, we will see the main steps in the exercise below.</p> <p>There are many different ways to create your webpages. We recommend using Mkdocs and Mkdocs materials as a framework to create a nice webpage in a simple manner. The folder templates that we used as an example in the previous exercise already contain everything you need to start a webpage. Nonetheless, you will need to understand the basics of MkDocs and MkDocs materials to design a webpage to your liking. MkDocs is a static webpage generator that is very easy to use, while MkDocs materials is an extension of the tool that gives you many more options to customize your website. Check out their webpages to get started!</p> <p>Exercise 5: make a project folder and publish a data analysis webpage</p> <ol> <li>Configure your main GitHub Page and its repo</li> </ol> <p>The first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps. After you have created the *organization/username*github.io, it is time to configure your <code>Project</code> repository webpage using MkDocs!</p> <ol> <li>Start a new project from cookiecutter or use one from the previous exercise.</li> </ol> <p>If you use a <code>Project</code> repo from the first exercise, go to the next paragraph. Using cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.</p> <p>Next, link your data of interest (or create a small fake dataset) and make an example of data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using <code>pip</code> allows you to directly add a Jupyter Notebook file to the <code>mkdocs.yml</code> navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.</p> <p>For the purposes of this exercise, we have already included a basic <code>index.md</code> markdown file that can serve as the intro page of your repo, and a <code>jupyter_example.ipynb</code> with some code in it. You are welcome to modify them further to test them out!</p> <ol> <li>Use MkDocs to create your webpage</li> </ol> <p>When you are happy with your files and are ready too publish them, make sure to add, commit and push the changes to the remote. Then, build up your webpage using MkDocs and the <code>mkdocs gh-deploy</code> command from the same directory where the <code>mkdocs.yml</code> file is. For example, if your <code>mkdocs.yml</code> for your <code>Project</code> folder is in <code>/Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml</code>, do <code>cd /Users/JARH/Projects/project1_JARH_20231010/</code> and then <code>mkdocs gh-deploy</code>.</p> <p>Finally, we only need to set up the GitHub <code>Project</code> repo settings.</p> <ol> <li>Publishing your GitHub Page</li> </ol> <p>Go to your GitHub repo settings and configure the Page section. Since you are using the <code>mkdocs gh-deploy</code> command to publish your site in the <code>gh-pages</code> branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website from:</p> <p></p> <ul> <li>Branch should be <code>gh-pages</code></li> <li>Folder should be <code>root</code></li> </ul> <p>After a couple of minutes, your webpage should be ready!</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#6-archive-github-repositories-on-zenodo","title":"6. Archive GitHub repositories on Zenodo","text":"<p>Archives are dedicated digital platforms designed for the secure storage, curation, and dissemination of scientific data. These repositories hold great importance in the research community as they serve as reliable archives for preserving valuable datasets. Their standardized formats and robust curation processes ensure the long-term accessibility and citability of research findings. Researchers worldwide rely on these repositories to share, discover, and validate scientific information, thereby fostering transparency, collaboration, and the advancement of knowledge across various domains of study.</p> <p>The next practical exercise will be to archive your <code>Project</code> folder that contains the data analyses performed on your NGS data in a repository like Zenodo. We can do this by linking your Zenodo account to your GitHub account.</p> <p>Archiving your NGS data</p> <p>In this practical lesson, we will only archive our data analyses in the <code>Project</code> folders. Your actual NGS data should be deposited in a domain-specific archive such as Gene Expression Omnibus (GEO) or Annotare. If you want to know more about these archives, check out this lesson</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#zenodo","title":"Zenodo","text":"<p>Zenodo[https://zenodo.org/] is an open-access digital repository designed to facilitate the archiving of scientific research outputs. It operates under the umbrella of the European Organization for Nuclear Research (CERN) and is supported by the European Commission. Zenodo accommodates a broad spectrum of research outputs, including datasets, papers, software, and multimedia files. This versatility makes it an invaluable resource for researchers across a wide array of domains, promoting transparency, collaboration, and the advancement of knowledge on a global scale.</p> <p>Operating on a user-friendly web platform, Zenodo allows researchers to easily upload, share, and preserve their research data and related materials. Upon deposit, each item is assigned a unique Digital Object Identifier (DOI), granting it a citable status and ensuring its long-term accessibility. Additionally, Zenodo provides robust metadata capabilities, enabling researchers to enrich their submissions with detailed contextual information. In addition, it allows you to link you GitHub account, providing a streamlined way to archive a specific release of your GitHub repository directly into Zenodo. This integration simplifies the process of preserving a snapshot of your project's progress for long-term accessibility and citation.</p> <p>Exercise 6: Archive a <code>Project</code> GitHub repo in Zenodo</p> <ol> <li>In order to archive your GitHub repos in Zenodo, you will first need to link your Zenodo and GitHub accounts.</li> <li>Once your accounts are linked, go to your Zenodo GitHub account settings and turn on the GitHub repository you want to archive. </li> <li>Creating a Zenodo archive is now as simple as making a release in your GitHub repository. Remember to make a proper tag! NOTE: If you make a release before enabling the GitHub repository in Zenodo, it will not appear in Zenodo! </li> <li>Zenodo will automatically detect the release and it should appear in your Zenodo upload page. </li> <li>This archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work. </li> </ol> <p>Before submitting your work in a journal, make sure to link your data analysis repository to Zenodo, get a DOI and cite it in your manuscript!</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"practical_workshop.html#wrap-up","title":"Wrap up","text":"<p>In this small workshop we have learned how improve the FAIRability of your data, as well as organizing and structuring it in a way that will be much more useful in the future. This advantages do not serve yourself only, but your teammates, group leader and the general scientific population! We hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to answer this form!</p> <p>FEEDBACK FORM</p>","tags":["Practical lessons","Exercises","DTU workshop"]},{"location":"keywords.html","title":"Keyword index","text":""},{"location":"keywords.html#keywords","title":"Keywords","text":"<p>Here's a lit of used keywords:</p>"},{"location":"keywords.html#bad-rdm","title":"Bad RDM","text":"<ul> <li>RDM intro</li> </ul>"},{"location":"keywords.html#bioinformatics-data","title":"Bioinformatics data","text":"<ul> <li>Next Generation Sequencing Data</li> </ul>"},{"location":"keywords.html#dmp","title":"DMP","text":"<ul> <li>Data Management Plans</li> </ul>"},{"location":"keywords.html#dtu-workshop","title":"DTU workshop","text":"<ul> <li>DTU workshop 2023</li> </ul>"},{"location":"keywords.html#data-life-cycle","title":"Data Life Cycle","text":"<ul> <li>Data Life Cycle</li> </ul>"},{"location":"keywords.html#data-management-plan","title":"Data Management Plan","text":"<ul> <li>Data Management Plans</li> </ul>"},{"location":"keywords.html#data-formats","title":"Data formats","text":"<ul> <li>Next Generation Sequencing Data</li> </ul>"},{"location":"keywords.html#exercises","title":"Exercises","text":"<ul> <li>DTU workshop 2023</li> </ul>"},{"location":"keywords.html#file-organization","title":"File organization","text":"<ul> <li>File structure and naming conventions</li> </ul>"},{"location":"keywords.html#file-structure","title":"File structure","text":"<ul> <li>File structure and naming conventions</li> </ul>"},{"location":"keywords.html#folder-templates","title":"Folder templates","text":"<ul> <li>File structure and naming conventions</li> </ul>"},{"location":"keywords.html#good-rdm","title":"Good RDM","text":"<ul> <li>RDM intro</li> </ul>"},{"location":"keywords.html#ngs","title":"NGS","text":"<ul> <li>Next Generation Sequencing Data</li> </ul>"},{"location":"keywords.html#naming-conventions","title":"Naming conventions","text":"<ul> <li>File structure and naming conventions</li> </ul>"},{"location":"keywords.html#practical-lessons","title":"Practical lessons","text":"<ul> <li>DTU workshop 2023</li> </ul>"},{"location":"keywords.html#rdm","title":"RDM","text":"<ul> <li>RDM intro</li> </ul>"},{"location":"keywords.html#authors","title":"authors","text":"<ul> <li>Contributors</li> </ul>"},{"location":"keywords.html#contributors","title":"contributors","text":"<ul> <li>Contributors</li> </ul>"}]}